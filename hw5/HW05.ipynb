{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AFEKWoh3p1Mv"
      },
      "source": [
        "# Homework Description\n",
        "- English to Chinese (Traditional) Translation\n",
        "  - Input: an English sentence         (e.g.\t\ttom is a student .)\n",
        "  - Output: the Chinese translation  (e.g. \t\t湯姆 是 個 學生 。)\n",
        "\n",
        "- TODO\n",
        "    - Train a simple RNN seq2seq to acheive translation\n",
        "    - Switch to transformer model to boost performance\n",
        "    - Apply Back-translation to furthur boost performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "3Vf1Q79XPQ3D"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sun Apr  3 12:21:24 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 495.29.05    Driver Version: 495.29.05    CUDA Version: 11.5     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  NVIDIA GeForce ...  On   | 00000000:65:00.0  On |                  N/A |\n",
            "|  0%   44C    P8    20W / 275W |    261MiB / 11177MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|    0   N/A  N/A      1099      G   /usr/lib/xorg/Xorg                 89MiB |\n",
            "|    0   N/A  N/A      1726      G   /usr/bin/gnome-shell               23MiB |\n",
            "|    0   N/A  N/A   1338107      G   /usr/lib/xorg/Xorg                 42MiB |\n",
            "|    0   N/A  N/A   1354911      G   ...mviewer/tv_bin/TeamViewer        2MiB |\n",
            "|    0   N/A  N/A   1888130      G   gnome-control-center                2MiB |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59neB_Sxp5Ub"
      },
      "source": [
        "# Download and import required packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "rRlFbfFRpZYT"
      },
      "outputs": [],
      "source": [
        "# !pip install 'torch>=1.6.0' editdistance matplotlib sacrebleu sacremoses sentencepiece tqdm wandb\n",
        "# !pip install --upgrade jupyter ipywidgets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "fSksMTdmp-Wt"
      },
      "outputs": [],
      "source": [
        "# !git clone https://github.com/pytorch/fairseq.git\n",
        "# !cd fairseq && git checkout 9a1c497\n",
        "# !pip install --upgrade ./fairseq/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "uRLTiuIuqGNc"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import pdb\n",
        "import pprint\n",
        "import logging\n",
        "import os\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils import data\n",
        "import numpy as np\n",
        "import tqdm.auto as tqdm\n",
        "from pathlib import Path\n",
        "from argparse import Namespace\n",
        "from fairseq import utils\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0n07Za1XqJzA"
      },
      "source": [
        "# Fix random seed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "xllxxyWxqI7s"
      },
      "outputs": [],
      "source": [
        "seed = 91322\n",
        "random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)  \n",
        "np.random.seed(seed)  \n",
        "torch.backends.cudnn.benchmark = False\n",
        "torch.backends.cudnn.deterministic = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N5ORDJ-2qdYw"
      },
      "source": [
        "# Dataset\n",
        "\n",
        "## En-Zh Bilingual Parallel Corpus\n",
        "* [TED2020](#reimers-2020-multilingual-sentence-bert)\n",
        "    - Raw: 398,066 (sentences)   \n",
        "    - Processed: 393,980 (sentences)\n",
        "    \n",
        "\n",
        "## Testdata\n",
        "- Size: 4,000 (sentences)\n",
        "- **Chinese translation is undisclosed. The provided (.zh) file is psuedo translation, each line is a '。'**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQw2mY4Dqkzd"
      },
      "source": [
        "## Dataset Download"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "SXT42xQtqijD"
      },
      "outputs": [],
      "source": [
        "data_dir = './DATA/rawdata'\n",
        "dataset_name = 'ted2020'\n",
        "urls = (\n",
        "    \"https://github.com/yuhsinchan/ML2022-HW5Dataset/releases/download/v1.0.2/ted2020.tgz\",\n",
        "    \"https://github.com/yuhsinchan/ML2022-HW5Dataset/releases/download/v1.0.2/test.tgz\",\n",
        ")\n",
        "file_names = (\n",
        "    'ted2020.tgz', # train & dev\n",
        "    'test.tgz', # test\n",
        ")\n",
        "prefix = Path(data_dir).absolute() / dataset_name\n",
        "\n",
        "prefix.mkdir(parents=True, exist_ok=True)\n",
        "# for u, f in zip(urls, file_names):\n",
        "#     path = prefix/f\n",
        "#     if not path.exists():\n",
        "#         !wget {u} -O {path}\n",
        "#     if path.suffix == \".tgz\":\n",
        "#         !tar -xvf {path} -C {prefix}\n",
        "#     elif path.suffix == \".zip\":\n",
        "#         !unzip -o {path} -d {prefix}\n",
        "# !mv {prefix/'raw.en'} {prefix/'train_dev.raw.en'}\n",
        "# !mv {prefix/'raw.zh'} {prefix/'train_dev.raw.zh'}\n",
        "# !mv {prefix/'test/test.en'} {prefix/'test.raw.en'}\n",
        "# !mv {prefix/'test/test.zh'} {prefix/'test.raw.zh'}\n",
        "# !rm -rf {prefix/'test'}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YLkJwNiFrIwZ"
      },
      "source": [
        "## Language"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "_uJYkCncrKJb"
      },
      "outputs": [],
      "source": [
        "src_lang = 'en'\n",
        "tgt_lang = 'zh'\n",
        "\n",
        "data_prefix = f'{prefix}/train_dev.raw'\n",
        "test_prefix = f'{prefix}/test.raw'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "0t2CPt1brOT3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Thank you so much, Chris.\n",
            "And it's truly a great honor to have the opportunity to come to this stage twice; I'm extremely grateful.\n",
            "I have been blown away by this conference, and I want to thank all of you for the many nice comments about what I had to say the other night.\n",
            "And I say that sincerely, partly because  I need that.\n",
            "Put yourselves in my position.\n",
            "非常謝謝你，克里斯。能有這個機會第二度踏上這個演講台\n",
            "真是一大榮幸。我非常感激。\n",
            "這個研討會給我留下了極為深刻的印象，我想感謝大家 對我之前演講的好評。\n",
            "我是由衷的想這麼說，有部份原因是因為 —— 我真的有需要!\n",
            "請你們設身處地為我想一想！\n"
          ]
        }
      ],
      "source": [
        "!head {data_prefix+'.'+src_lang} -n 5\n",
        "!head {data_prefix+'.'+tgt_lang} -n 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pRoE9UK7r1gY"
      },
      "source": [
        "## Preprocess files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "3tzFwtnFrle3"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def strQ2B(ustring):\n",
        "    \"\"\"Full width -> half width\"\"\"\n",
        "    # reference:https://ithelp.ithome.com.tw/articles/10233122\n",
        "    ss = []\n",
        "    for s in ustring:\n",
        "        rstring = \"\"\n",
        "        for uchar in s:\n",
        "            inside_code = ord(uchar)\n",
        "            if inside_code == 12288:  # Full width space: direct conversion\n",
        "                inside_code = 32\n",
        "            elif (inside_code >= 65281 and inside_code <= 65374):  # Full width chars (except space) conversion\n",
        "                inside_code -= 65248\n",
        "            rstring += chr(inside_code)\n",
        "        ss.append(rstring)\n",
        "    return ''.join(ss)\n",
        "                \n",
        "def clean_s(s, lang):\n",
        "    if lang == 'en':\n",
        "        s = re.sub(r\"\\([^()]*\\)\", \"\", s) # remove ([text])\n",
        "        s = s.replace('-', '') # remove '-'\n",
        "        s = re.sub('([.,;!?()\\\"])', r' \\1 ', s) # keep punctuation\n",
        "    elif lang == 'zh':\n",
        "        s = strQ2B(s) # Q2B\n",
        "        s = re.sub(r\"\\([^()]*\\)\", \"\", s) # remove ([text])\n",
        "        s = s.replace(' ', '')\n",
        "        s = s.replace('—', '')\n",
        "        s = s.replace('“', '\"')\n",
        "        s = s.replace('”', '\"')\n",
        "        s = s.replace('_', '')\n",
        "        s = re.sub('([。,;!?()\\\"~「」])', r' \\1 ', s) # keep punctuation\n",
        "    s = ' '.join(s.strip().split())\n",
        "    return s\n",
        "\n",
        "def len_s(s, lang):\n",
        "    if lang == 'zh':\n",
        "        return len(s)\n",
        "    return len(s.split())\n",
        "\n",
        "def clean_corpus(prefix, l1, l2, ratio=9, max_len=1000, min_len=1):\n",
        "    if Path(f'{prefix}.clean.{l1}').exists() and Path(f'{prefix}.clean.{l2}').exists():\n",
        "        print(f'{prefix}.clean.{l1} & {l2} exists. skipping clean.')\n",
        "        return\n",
        "    with open(f'{prefix}.{l1}', 'r') as l1_in_f:\n",
        "        with open(f'{prefix}.{l2}', 'r') as l2_in_f:\n",
        "            with open(f'{prefix}.clean.{l1}', 'w') as l1_out_f:\n",
        "                with open(f'{prefix}.clean.{l2}', 'w') as l2_out_f:\n",
        "                    for s1 in l1_in_f:\n",
        "                        s1 = s1.strip()\n",
        "                        s2 = l2_in_f.readline().strip()\n",
        "                        s1 = clean_s(s1, l1)\n",
        "                        s2 = clean_s(s2, l2)\n",
        "                        s1_len = len_s(s1, l1)\n",
        "                        s2_len = len_s(s2, l2)\n",
        "                        if min_len > 0: # remove short sentence\n",
        "                            if s1_len < min_len or s2_len < min_len:\n",
        "                                continue\n",
        "                        if max_len > 0: # remove long sentence\n",
        "                            if s1_len > max_len or s2_len > max_len:\n",
        "                                continue\n",
        "                        if ratio > 0: # remove by ratio of length\n",
        "                            if s1_len/s2_len > ratio or s2_len/s1_len > ratio:\n",
        "                                continue\n",
        "                        print(s1, file=l1_out_f)\n",
        "                        print(s2, file=l2_out_f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "h_i8b1PRr9Nf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/home/haoyu/Desktop/2022ML/hw5/DATA/rawdata/ted2020/train_dev.raw.clean.en & zh exists. skipping clean.\n",
            "/home/haoyu/Desktop/2022ML/hw5/DATA/rawdata/ted2020/test.raw.clean.en & zh exists. skipping clean.\n"
          ]
        }
      ],
      "source": [
        "clean_corpus(data_prefix, src_lang, tgt_lang)\n",
        "clean_corpus(test_prefix, src_lang, tgt_lang, ratio=-1, min_len=-1, max_len=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "gjT3XCy9r_rj"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Thank you so much , Chris .\n",
            "And it's truly a great honor to have the opportunity to come to this stage twice ; I'm extremely grateful .\n",
            "I have been blown away by this conference , and I want to thank all of you for the many nice comments about what I had to say the other night .\n",
            "And I say that sincerely , partly because I need that .\n",
            "Put yourselves in my position .\n",
            "非常謝謝你 , 克里斯 。 能有這個機會第二度踏上這個演講台\n",
            "真是一大榮幸 。 我非常感激 。\n",
            "這個研討會給我留下了極為深刻的印象 , 我想感謝大家對我之前演講的好評 。\n",
            "我是由衷的想這麼說 , 有部份原因是因為我真的有需要 !\n",
            "請你們設身處地為我想一想 !\n"
          ]
        }
      ],
      "source": [
        "!head {data_prefix+'.clean.'+src_lang} -n 5\n",
        "!head {data_prefix+'.clean.'+tgt_lang} -n 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nKb4u67-sT_Z"
      },
      "source": [
        "## Split into train/valid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "AuFKeDz3sGHL"
      },
      "outputs": [],
      "source": [
        "valid_ratio = 0.01 # 3000~4000 would suffice\n",
        "train_ratio = 1 - valid_ratio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "QR2NVldqsXyY"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train/valid splits exists. skipping split.\n"
          ]
        }
      ],
      "source": [
        "if (prefix/f'train.clean.{src_lang}').exists() \\\n",
        "and (prefix/f'train.clean.{tgt_lang}').exists() \\\n",
        "and (prefix/f'valid.clean.{src_lang}').exists() \\\n",
        "and (prefix/f'valid.clean.{tgt_lang}').exists():\n",
        "    print(f'train/valid splits exists. skipping split.')\n",
        "else:\n",
        "    line_num = sum(1 for line in open(f'{data_prefix}.clean.{src_lang}'))\n",
        "    labels = list(range(line_num))\n",
        "    random.shuffle(labels)\n",
        "    for lang in [src_lang, tgt_lang]:\n",
        "        train_f = open(os.path.join(data_dir, dataset_name, f'train.clean.{lang}'), 'w')\n",
        "        valid_f = open(os.path.join(data_dir, dataset_name, f'valid.clean.{lang}'), 'w')\n",
        "        count = 0\n",
        "        for line in open(f'{data_prefix}.clean.{lang}', 'r'):\n",
        "            if labels[count]/line_num < train_ratio:\n",
        "                train_f.write(line)\n",
        "            else:\n",
        "                valid_f.write(line)\n",
        "            count += 1\n",
        "        train_f.close()\n",
        "        valid_f.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n1rwQysTsdJq"
      },
      "source": [
        "## Subword Units \n",
        "Out of vocabulary (OOV) has been a major problem in machine translation. This can be alleviated by using subword units.\n",
        "- We will use the [sentencepiece](#kudo-richardson-2018-sentencepiece) package\n",
        "- select 'unigram' or 'byte-pair encoding (BPE)' algorithm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Ecwllsa7sZRA"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/home/haoyu/Desktop/2022ML/hw5/DATA/rawdata/ted2020/spm8000.model exists. skipping spm_train.\n"
          ]
        }
      ],
      "source": [
        "import sentencepiece as spm\n",
        "vocab_size = 8000\n",
        "if (prefix/f'spm{vocab_size}.model').exists():\n",
        "    print(f'{prefix}/spm{vocab_size}.model exists. skipping spm_train.')\n",
        "else:\n",
        "    spm.SentencePieceTrainer.train(\n",
        "        input=','.join([f'{prefix}/train.clean.{src_lang}',\n",
        "                        f'{prefix}/valid.clean.{src_lang}',\n",
        "                        f'{prefix}/train.clean.{tgt_lang}',\n",
        "                        f'{prefix}/valid.clean.{tgt_lang}']),\n",
        "        model_prefix=prefix/f'spm{vocab_size}',\n",
        "        vocab_size=vocab_size,\n",
        "        character_coverage=1,\n",
        "        model_type='unigram', # 'bpe' works as well\n",
        "        input_sentence_size=1e6,\n",
        "        shuffle_input_sentence=True,\n",
        "        normalization_rule_name='nmt_nfkc_cf',\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "lQPRNldqse_V"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/home/haoyu/Desktop/2022ML/hw5/DATA/rawdata/ted2020/train.en exists. skipping spm_encode.\n",
            "/home/haoyu/Desktop/2022ML/hw5/DATA/rawdata/ted2020/train.zh exists. skipping spm_encode.\n",
            "/home/haoyu/Desktop/2022ML/hw5/DATA/rawdata/ted2020/valid.en exists. skipping spm_encode.\n",
            "/home/haoyu/Desktop/2022ML/hw5/DATA/rawdata/ted2020/valid.zh exists. skipping spm_encode.\n",
            "/home/haoyu/Desktop/2022ML/hw5/DATA/rawdata/ted2020/test.en exists. skipping spm_encode.\n",
            "/home/haoyu/Desktop/2022ML/hw5/DATA/rawdata/ted2020/test.zh exists. skipping spm_encode.\n"
          ]
        }
      ],
      "source": [
        "spm_model = spm.SentencePieceProcessor(model_file=str(prefix/f'spm{vocab_size}.model'))\n",
        "in_tag = {\n",
        "    'train': 'train.clean',\n",
        "    'valid': 'valid.clean',\n",
        "    'test': 'test.raw.clean',\n",
        "}\n",
        "for split in ['train', 'valid', 'test']:\n",
        "    for lang in [src_lang, tgt_lang]:\n",
        "        out_path = prefix/f'{split}.{lang}'\n",
        "        if out_path.exists():\n",
        "            print(f\"{out_path} exists. skipping spm_encode.\")\n",
        "        else:\n",
        "            with open(prefix/f'{split}.{lang}', 'w') as out_f:\n",
        "                with open(prefix/f'{in_tag[split]}.{lang}', 'r') as in_f:\n",
        "                    for line in in_f:\n",
        "                        line = line.strip()\n",
        "                        tok = spm_model.encode(line, out_type=str)\n",
        "                        print(' '.join(tok), file=out_f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "4j6lXHjAsjXa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "▁thank ▁you ▁so ▁much ▁, ▁chris ▁.\n",
            "▁and ▁it ' s ▁ t ru ly ▁a ▁great ▁ho n or ▁to ▁have ▁the ▁ op port un ity ▁to ▁come ▁to ▁this ▁st age ▁ t wi ce ▁; ▁i ' m ▁ex t re me ly ▁gr ate ful ▁.\n",
            "▁i ▁have ▁been ▁ bl ow n ▁away ▁by ▁this ▁con fer ence ▁, ▁and ▁i ▁want ▁to ▁thank ▁all ▁of ▁you ▁for ▁the ▁many ▁ ni ce ▁ com ment s ▁about ▁what ▁i ▁had ▁to ▁say ▁the ▁other ▁night ▁.\n",
            "▁and ▁i ▁say ▁that ▁since re ly ▁, ▁part ly ▁because ▁i ▁need ▁that ▁.\n",
            "▁put ▁your s el ve s ▁in ▁my ▁po s ition ▁.\n",
            "▁ 非常 謝 謝 你 ▁, ▁ 克 里 斯 ▁。 ▁ 能 有 這個 機會 第二 度 踏 上 這個 演講 台\n",
            "▁ 真 是 一 大 榮 幸 ▁。 ▁我 非常 感 激 ▁。\n",
            "▁這個 研 討 會 給我 留 下 了 極 為 深 刻 的 印 象 ▁, ▁我想 感 謝 大家 對我 之前 演講 的 好 評 ▁。\n",
            "▁我 是由 衷 的 想 這麼 說 ▁, ▁有 部份 原因 是因為 我 真的 有 需要 ▁!\n",
            "▁ 請 你們 設 身 處 地 為 我想 一 想 ▁!\n"
          ]
        }
      ],
      "source": [
        "!head {data_dir+'/'+dataset_name+'/train.'+src_lang} -n 5\n",
        "!head {data_dir+'/'+dataset_name+'/train.'+tgt_lang} -n 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59si_C0Wsms7"
      },
      "source": [
        "## Binarize the data with fairseq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "w-cHVLSpsknh"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DATA/data-bin/ted2020 exists, will not overwrite!\n"
          ]
        }
      ],
      "source": [
        "binpath = Path('./DATA/data-bin', dataset_name)\n",
        "if binpath.exists():\n",
        "    print(binpath, \"exists, will not overwrite!\")\n",
        "else:\n",
        "    !python3 -m fairseq_cli.preprocess \\\n",
        "        --source-lang {src_lang}\\\n",
        "        --target-lang {tgt_lang}\\\n",
        "        --trainpref {prefix/'train'}\\\n",
        "        --validpref {prefix/'valid'}\\\n",
        "        --testpref {prefix/'test'}\\\n",
        "        --destdir {binpath}\\\n",
        "        --joined-dictionary\\\n",
        "        --workers 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "szMuH1SWLPWA"
      },
      "source": [
        "# Configuration for experiments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "5Luz3_tVLUxs"
      },
      "outputs": [],
      "source": [
        "config = Namespace(\n",
        "    datadir = \"./DATA/data-bin/ted2020\",\n",
        "    savedir = \"./checkpoints/rnn\",\n",
        "    source_lang = \"en\",\n",
        "    target_lang = \"zh\",\n",
        "    \n",
        "    # cpu threads when fetching & processing data.\n",
        "    num_workers=2,  \n",
        "    # batch size in terms of tokens. gradient accumulation increases the effective batchsize.\n",
        "    max_tokens=8192,\n",
        "    accum_steps=2,\n",
        "    \n",
        "    # the lr s calculated from Noam lr scheduler. you can tune the maximum lr by this factor.\n",
        "    lr_factor=2.,\n",
        "    lr_warmup=4000,\n",
        "    \n",
        "    # clipping gradient norm helps alleviate gradient exploding\n",
        "    clip_norm=1.0,\n",
        "    \n",
        "    # maximum epochs for training\n",
        "    max_epoch=30,\n",
        "    start_epoch=1,\n",
        "    \n",
        "    # beam size for beam search\n",
        "    beam=5, \n",
        "    # generate sequences of maximum length ax + b, where x is the source length\n",
        "    max_len_a=1.2, \n",
        "    max_len_b=10, \n",
        "    # when decoding, post process sentence by removing sentencepiece symbols and jieba tokenization.\n",
        "    post_process = \"sentencepiece\",\n",
        "    \n",
        "    # checkpoints\n",
        "    keep_last_epochs=5,\n",
        "    resume=None, # if resume from checkpoint name (under config.savedir)\n",
        "    \n",
        "    # logging\n",
        "    use_wandb=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cjrJFvyQLg86"
      },
      "source": [
        "# Logging\n",
        "- logging package logs ordinary messages\n",
        "- wandb logs the loss, bleu, etc. in the training process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "-ZiMyDWALbDk"
      },
      "outputs": [],
      "source": [
        "logging.basicConfig(\n",
        "    format=\"%(asctime)s | %(levelname)s | %(name)s | %(message)s\",\n",
        "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
        "    level=\"INFO\", # \"DEBUG\" \"WARNING\" \"ERROR\"\n",
        "    stream=sys.stdout,\n",
        ")\n",
        "proj = \"hw5.seq2seq\"\n",
        "logger = logging.getLogger(proj)\n",
        "if config.use_wandb:\n",
        "    import wandb\n",
        "    wandb.init(project=proj, name=Path(config.savedir).stem, config=config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNoSkK45Lmqc"
      },
      "source": [
        "# CUDA Environments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "oqrsbmcoLqMl"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-03 12:21:32 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
            "2022-04-03 12:21:32 | INFO | fairseq.utils | rank   0: capabilities =  6.1  ; total memory = 10.915 GB ; name = NVIDIA GeForce GTX 1080 Ti              \n",
            "2022-04-03 12:21:32 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n"
          ]
        }
      ],
      "source": [
        "cuda_env = utils.CudaEnvironment()\n",
        "utils.CudaEnvironment.pretty_print_cuda_env_list([cuda_env])\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TbJuBIHLLt2D"
      },
      "source": [
        "# Dataloading"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oOpG4EBRLwe_"
      },
      "source": [
        "## We borrow the TranslationTask from fairseq\n",
        "* used to load the binarized data created above\n",
        "* well-implemented data iterator (dataloader)\n",
        "* built-in task.source_dictionary and task.target_dictionary are also handy\n",
        "* well-implemented beach search decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "3gSEy1uFLvVs"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-03 12:21:33 | INFO | fairseq.tasks.translation | [en] dictionary: 8000 types\n",
            "2022-04-03 12:21:33 | INFO | fairseq.tasks.translation | [zh] dictionary: 8000 types\n"
          ]
        }
      ],
      "source": [
        "from fairseq.tasks.translation import TranslationConfig, TranslationTask\n",
        "\n",
        "## setup task\n",
        "task_cfg = TranslationConfig(\n",
        "    data=config.datadir,\n",
        "    source_lang=config.source_lang,\n",
        "    target_lang=config.target_lang,\n",
        "    train_subset=\"train\",\n",
        "    required_seq_len_multiple=8,\n",
        "    dataset_impl=\"mmap\",\n",
        "    upsample_primary=1,\n",
        ")\n",
        "task = TranslationTask.setup_task(task_cfg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "mR7Bhov7L4IU"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-03 12:21:33 | INFO | hw5.seq2seq | loading data for epoch 1\n",
            "2022-04-03 12:21:33 | INFO | fairseq.data.data_utils | loaded 390,041 examples from: ./DATA/data-bin/ted2020/train.en-zh.en\n",
            "2022-04-03 12:21:33 | INFO | fairseq.data.data_utils | loaded 390,041 examples from: ./DATA/data-bin/ted2020/train.en-zh.zh\n",
            "2022-04-03 12:21:33 | INFO | fairseq.tasks.translation | ./DATA/data-bin/ted2020 train en-zh 390041 examples\n",
            "2022-04-03 12:21:33 | INFO | fairseq.data.data_utils | loaded 3,939 examples from: ./DATA/data-bin/ted2020/valid.en-zh.en\n",
            "2022-04-03 12:21:33 | INFO | fairseq.data.data_utils | loaded 3,939 examples from: ./DATA/data-bin/ted2020/valid.en-zh.zh\n",
            "2022-04-03 12:21:33 | INFO | fairseq.tasks.translation | ./DATA/data-bin/ted2020 valid en-zh 3939 examples\n"
          ]
        }
      ],
      "source": [
        "logger.info(\"loading data for epoch 1\")\n",
        "task.load_dataset(split=\"train\", epoch=1, combine=True) # combine if you have back-translation data.\n",
        "task.load_dataset(split=\"valid\", epoch=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "P0BCEm_9L6ig"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'id': 1,\n",
            " 'source': tensor([  18,   14,    6, 2234,   60,   19,   80,    5,  256,   16,  405, 1407,\n",
            "        1706,    7,    2]),\n",
            " 'target': tensor([ 140,  690,   28,  270,   45,  151, 1142,  660,  606,  369, 3114, 2434,\n",
            "        1434,  192,    2])}\n",
            "\"Source: that's exactly what i do optical mind control .\"\n",
            "'Target: 這實在就是我所做的--光學操控思想'\n"
          ]
        }
      ],
      "source": [
        "sample = task.dataset(\"valid\")[1]\n",
        "pprint.pprint(sample)\n",
        "pprint.pprint(\n",
        "    \"Source: \" + \\\n",
        "    task.source_dictionary.string(\n",
        "        sample['source'],\n",
        "        config.post_process,\n",
        "    )\n",
        ")\n",
        "pprint.pprint(\n",
        "    \"Target: \" + \\\n",
        "    task.target_dictionary.string(\n",
        "        sample['target'],\n",
        "        config.post_process,\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UcfCVa2FMBSE"
      },
      "source": [
        "# Dataset iterator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yBvc-B_6MKZM"
      },
      "source": [
        "* Controls every batch to contain no more than N tokens, which optimizes GPU memory efficiency\n",
        "* Shuffles the training set for every epoch\n",
        "* Ignore sentences exceeding maximum length\n",
        "* Pad all sentences in a batch to the same length, which enables parallel computing by GPU\n",
        "* Add eos and shift one token\n",
        "    - teacher forcing: to train the model to predict the next token based on prefix, we feed the right shifted target sequence as the decoder input.\n",
        "    - generally, prepending bos to the target would do the job (as shown below)\n",
        "![seq2seq](https://i.imgur.com/0zeDyuI.png)\n",
        "    - in fairseq however, this is done by moving the eos token to the begining. Empirically, this has the same effect. For instance:\n",
        "    ```\n",
        "    # output target (target) and Decoder input (prev_output_tokens): \n",
        "                   eos = 2\n",
        "                target = 419,  711,  238,  888,  792,   60,  968,    8,    2\n",
        "    prev_output_tokens = 2,  419,  711,  238,  888,  792,   60,  968,    8\n",
        "    ```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "OWFJFmCnMDXW"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-03 12:21:35 | WARNING | fairseq.tasks.fairseq_task | 2,532 samples have invalid sizes and will be skipped, max_positions=(20, 20), first few sample ids=[29, 135, 2444, 3058, 682, 731, 235, 559, 3383, 1558]\n"
          ]
        },
        {
          "data": {
            "text/plain": "{'id': tensor([853]),\n 'nsentences': 1,\n 'ntokens': 14,\n 'net_input': {'src_tokens': tensor([[   1,    1,    1,    1,  466,   14,    6,  178,  150,   22,   63,   80,\n             20,   61,  896,    6,    6,  144,   29, 2249,   42,   25,    7,    2]]),\n  'src_lengths': tensor([20]),\n  'prev_output_tokens': tensor([[   2,   53,   78, 1255,    4,    5, 1245,  555,  891,  369,  118,  162,\n           1518,  380,    1,    1]])},\n 'target': tensor([[  53,   78, 1255,    4,    5, 1245,  555,  891,  369,  118,  162, 1518,\n           380,    2,    1,    1]])}"
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def load_data_iterator(task, split, epoch=1, max_tokens=4000, num_workers=1, cached=True):\n",
        "    batch_iterator = task.get_batch_iterator(\n",
        "        dataset=task.dataset(split),\n",
        "        max_tokens=max_tokens,\n",
        "        max_sentences=None,\n",
        "        max_positions=utils.resolve_max_positions(\n",
        "            task.max_positions(),\n",
        "            max_tokens,\n",
        "        ),\n",
        "        ignore_invalid_inputs=True,\n",
        "        seed=seed,\n",
        "        num_workers=num_workers,\n",
        "        epoch=epoch,\n",
        "        disable_iterator_cache=not cached,\n",
        "        # Set this to False to speed up. However, if set to False, changing max_tokens beyond \n",
        "        # first call of this method has no effect. \n",
        "    )\n",
        "    return batch_iterator\n",
        "\n",
        "demo_epoch_obj = load_data_iterator(task, \"valid\", epoch=1, max_tokens=20, num_workers=1, cached=False)\n",
        "demo_iter = demo_epoch_obj.next_epoch_itr(shuffle=True)\n",
        "sample = next(demo_iter)\n",
        "sample"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p86K-0g7Me4M"
      },
      "source": [
        "* each batch is a python dict, with string key and Tensor value. Contents are described below:\n",
        "```python\n",
        "batch = {\n",
        "    \"id\": id, # id for each example \n",
        "    \"nsentences\": len(samples), # batch size (sentences)\n",
        "    \"ntokens\": ntokens, # batch size (tokens)\n",
        "    \"net_input\": {\n",
        "        \"src_tokens\": src_tokens, # sequence in source language\n",
        "        \"src_lengths\": src_lengths, # sequence length of each example before padding\n",
        "        \"prev_output_tokens\": prev_output_tokens, # right shifted target, as mentioned above.\n",
        "    },\n",
        "    \"target\": target, # target sequence\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9EyDBE5ZMkFZ"
      },
      "source": [
        "# Model Architecture\n",
        "* We again inherit fairseq's encoder, decoder and model, so that in the testing phase we can directly leverage fairseq's beam search decoder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "Hzh74qLIMfW_"
      },
      "outputs": [],
      "source": [
        "from fairseq.models import (\n",
        "    FairseqEncoder, \n",
        "    FairseqIncrementalDecoder,\n",
        "    FairseqEncoderDecoderModel\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ZlE_1JnMv56"
      },
      "source": [
        "## Attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZSFSKt_ZMzgh"
      },
      "source": [
        "- When the input sequence is long, \"content vector\" alone cannot accurately represent the whole sequence, attention mechanism can provide the Decoder more information.\n",
        "- According to the **Decoder embeddings** of the current timestep, match the **Encoder outputs** with decoder embeddings to determine correlation, and then sum the Encoder outputs weighted by the correlation as the input to **Decoder** RNN.\n",
        "- Common attention implementations use neural network / dot product as the correlation between **query** (decoder embeddings) and **key** (Encoder outputs), followed by **softmax**  to obtain a distribution, and finally **values** (Encoder outputs) is **weighted sum**-ed by said distribution.\n",
        "\n",
        "- Parameters:\n",
        "  - *input_embed_dim*: dimensionality of key, should be that of the vector in decoder to attend others\n",
        "  - *source_embed_dim*: dimensionality of query, should be that of the vector to be attended to (encoder outputs)\n",
        "  - *output_embed_dim*: dimensionality of value, should be that of the vector after attention, expected by the next layer\n",
        "\n",
        "- Inputs: \n",
        "    - *inputs*: is the key, the vector to attend to others\n",
        "    - *encoder_outputs*:  is the query/value, the vector to be attended to\n",
        "    - *encoder_padding_mask*: this tells the decoder which position to ignore\n",
        "- Outputs: \n",
        "    - *output*: the context vector after attention\n",
        "    - *attention score*: the attention distribution\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "1Atf_YuCMyyF"
      },
      "outputs": [],
      "source": [
        "class AttentionLayer(nn.Module):\n",
        "    def __init__(self, input_embed_dim, source_embed_dim, output_embed_dim, bias=False):\n",
        "        super().__init__()\n",
        "\n",
        "        self.input_proj = nn.Linear(input_embed_dim, source_embed_dim, bias=bias)\n",
        "        self.output_proj = nn.Linear(\n",
        "            input_embed_dim + source_embed_dim, output_embed_dim, bias=bias\n",
        "        )\n",
        "\n",
        "    def forward(self, inputs, encoder_outputs, encoder_padding_mask):\n",
        "        # inputs: T, B, dim\n",
        "        # encoder_outputs: S x B x dim\n",
        "        # padding mask:  S x B\n",
        "        \n",
        "        # convert all to batch first\n",
        "        inputs = inputs.transpose(1,0) # B, T, dim\n",
        "        encoder_outputs = encoder_outputs.transpose(1,0) # B, S, dim\n",
        "        encoder_padding_mask = encoder_padding_mask.transpose(1,0) # B, S\n",
        "        \n",
        "        # project to the dimensionality of encoder_outputs\n",
        "        x = self.input_proj(inputs)\n",
        "\n",
        "        # compute attention\n",
        "        # (B, T, dim) x (B, dim, S) = (B, T, S)\n",
        "        attn_scores = torch.bmm(x, encoder_outputs.transpose(1,2))\n",
        "\n",
        "        # cancel the attention at positions corresponding to padding\n",
        "        if encoder_padding_mask is not None:\n",
        "            # leveraging broadcast  B, S -> (B, 1, S)\n",
        "            encoder_padding_mask = encoder_padding_mask.unsqueeze(1)\n",
        "            attn_scores = (\n",
        "                attn_scores.float()\n",
        "                .masked_fill_(encoder_padding_mask, float(\"-inf\"))\n",
        "                .type_as(attn_scores)\n",
        "            )  # FP16 support: cast to float and back\n",
        "\n",
        "        # softmax on the dimension corresponding to source sequence\n",
        "        attn_scores = F.softmax(attn_scores, dim=-1)\n",
        "\n",
        "        # shape (B, T, S) x (B, S, dim) = (B, T, dim) weighted sum\n",
        "        x = torch.bmm(attn_scores, encoder_outputs)\n",
        "\n",
        "        # (B, T, dim)\n",
        "        x = torch.cat((x, inputs), dim=-1)\n",
        "        x = torch.tanh(self.output_proj(x)) # concat + linear + tanh\n",
        "        \n",
        "        # restore shape (B, T, dim) -> (T, B, dim)\n",
        "        return x.transpose(1,0), attn_scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UDAPmxjRNEEL"
      },
      "source": [
        "## Seq2Seq\n",
        "- Composed of **Encoder** and **Decoder**\n",
        "- Recieves inputs and pass to **Encoder** \n",
        "- Pass the outputs from **Encoder** to **Decoder**\n",
        "- **Decoder** will decode according to outputs of previous timesteps as well as **Encoder** outputs  \n",
        "- Once done decoding, return the **Decoder** outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "oRwKdLa0NEU6"
      },
      "outputs": [],
      "source": [
        "class Seq2Seq(FairseqEncoderDecoderModel):\n",
        "    def __init__(self, args, encoder, decoder):\n",
        "        super().__init__(encoder, decoder)\n",
        "        self.args = args\n",
        "    \n",
        "    def forward(\n",
        "        self,\n",
        "        src_tokens,\n",
        "        src_lengths,\n",
        "        prev_output_tokens,\n",
        "        return_all_hiddens: bool = True,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Run the forward pass for an encoder-decoder model.\n",
        "        \"\"\"\n",
        "        encoder_out = self.encoder(\n",
        "            src_tokens, src_lengths=src_lengths, return_all_hiddens=return_all_hiddens\n",
        "        )\n",
        "        logits, extra = self.decoder(\n",
        "            prev_output_tokens,\n",
        "            encoder_out=encoder_out,\n",
        "            src_lengths=src_lengths,\n",
        "            return_all_hiddens=return_all_hiddens,\n",
        "        )\n",
        "        return logits, extra"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zu3C2JfqNHzk"
      },
      "source": [
        "# Model Initialization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "nyI9FOx-NJ2m"
      },
      "outputs": [],
      "source": [
        "# # HINT: transformer architecture\n",
        "from fairseq.models.transformer import (\n",
        "    TransformerEncoder, \n",
        "    TransformerDecoder,\n",
        ")\n",
        "\n",
        "def build_model(args, task):\n",
        "    \"\"\" build a model instance based on hyperparameters \"\"\"\n",
        "    src_dict, tgt_dict = task.source_dictionary, task.target_dictionary\n",
        "\n",
        "    # token embeddings\n",
        "    encoder_embed_tokens = nn.Embedding(len(src_dict), args.encoder_embed_dim, src_dict.pad())\n",
        "    decoder_embed_tokens = nn.Embedding(len(tgt_dict), args.decoder_embed_dim, tgt_dict.pad())\n",
        "    \n",
        "    # encoder decoder\n",
        "    # HINT: TODO: switch to TransformerEncoder & TransformerDecoder\n",
        "    # encoder = RNNEncoder(args, src_dict, encoder_embed_tokens)\n",
        "    # decoder = RNNDecoder(args, tgt_dict, decoder_embed_tokens)\n",
        "    encoder = TransformerEncoder(args, src_dict, encoder_embed_tokens)\n",
        "    decoder = TransformerDecoder(args, tgt_dict, decoder_embed_tokens)\n",
        "\n",
        "    # sequence to sequence model\n",
        "    model = Seq2Seq(args, encoder, decoder)\n",
        "    \n",
        "    # initialization for seq2seq model is important, requires extra handling\n",
        "    def init_params(module):\n",
        "        from fairseq.modules import MultiheadAttention\n",
        "        if isinstance(module, nn.Linear):\n",
        "            module.weight.data.normal_(mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                module.bias.data.zero_()\n",
        "        if isinstance(module, nn.Embedding):\n",
        "            module.weight.data.normal_(mean=0.0, std=0.02)\n",
        "            if module.padding_idx is not None:\n",
        "                module.weight.data[module.padding_idx].zero_()\n",
        "        if isinstance(module, MultiheadAttention):\n",
        "            module.q_proj.weight.data.normal_(mean=0.0, std=0.02)\n",
        "            module.k_proj.weight.data.normal_(mean=0.0, std=0.02)\n",
        "            module.v_proj.weight.data.normal_(mean=0.0, std=0.02)\n",
        "        if isinstance(module, nn.RNNBase):\n",
        "            for name, param in module.named_parameters():\n",
        "                if \"weight\" in name or \"bias\" in name:\n",
        "                    param.data.uniform_(-0.1, 0.1)\n",
        "            \n",
        "    # weight initialization\n",
        "    model.apply(init_params)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ce5n4eS7NQNy"
      },
      "source": [
        "## Architecture Related Configuration\n",
        "\n",
        "For strong baseline, please refer to the hyperparameters for *transformer-base* in Table 3 in [Attention is all you need](#vaswani2017)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "Cyn30VoGNT6N"
      },
      "outputs": [],
      "source": [
        "arch_args = Namespace(\n",
        "    encoder_embed_dim=512,\n",
        "    encoder_ffn_embed_dim=2048,\n",
        "    encoder_layers=6,\n",
        "    decoder_embed_dim=512,\n",
        "    decoder_ffn_embed_dim=2048,\n",
        "    decoder_layers=6,\n",
        "    share_decoder_input_output_embed=True,\n",
        "    dropout=0.1,\n",
        ")\n",
        "\n",
        "# HINT: these patches on parameters for Transformer\n",
        "def add_transformer_args(args):\n",
        "    args.encoder_attention_heads=4\n",
        "    args.encoder_normalize_before=True\n",
        "    \n",
        "    args.decoder_attention_heads=4\n",
        "    args.decoder_normalize_before=True\n",
        "    \n",
        "    args.activation_fn=\"relu\"\n",
        "    args.max_source_positions=1024\n",
        "    args.max_target_positions=1024\n",
        "    \n",
        "    # patches on default parameters for Transformer (those not set above)\n",
        "    from fairseq.models.transformer import base_architecture\n",
        "    base_architecture(arch_args)\n",
        "\n",
        "add_transformer_args(arch_args)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "Nbb76QLCNZZZ"
      },
      "outputs": [],
      "source": [
        "if config.use_wandb:\n",
        "    wandb.config.update(vars(arch_args))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "7ZWfxsCDNatH"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-03 12:21:40 | INFO | hw5.seq2seq | Seq2Seq(\n",
            "  (encoder): TransformerEncoder(\n",
            "    (dropout_module): FairseqDropout()\n",
            "    (embed_tokens): Embedding(8000, 512, padding_idx=1)\n",
            "    (embed_positions): SinusoidalPositionalEmbedding()\n",
            "    (layers): ModuleList(\n",
            "      (0): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (1): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (2): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (3): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (4): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (5): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "    (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "  )\n",
            "  (decoder): TransformerDecoder(\n",
            "    (dropout_module): FairseqDropout()\n",
            "    (embed_tokens): Embedding(8000, 512, padding_idx=1)\n",
            "    (embed_positions): SinusoidalPositionalEmbedding()\n",
            "    (layers): ModuleList(\n",
            "      (0): TransformerDecoderLayer(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (1): TransformerDecoderLayer(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (2): TransformerDecoderLayer(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (3): TransformerDecoderLayer(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (4): TransformerDecoderLayer(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (5): TransformerDecoderLayer(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "    (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "    (output_projection): Linear(in_features=512, out_features=8000, bias=False)\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "model = build_model(arch_args, task)\n",
        "logger.info(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aHll7GRNNdqc"
      },
      "source": [
        "# Optimization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rUB9f1WCNgMH"
      },
      "source": [
        "## Loss: Label Smoothing Regularization\n",
        "* let the model learn to generate less concentrated distribution, and prevent over-confidence\n",
        "* sometimes the ground truth may not be the only answer. thus, when calculating loss, we reserve some probability for incorrect labels\n",
        "* avoids overfitting\n",
        "\n",
        "code [source](https://fairseq.readthedocs.io/en/latest/_modules/fairseq/criterions/label_smoothed_cross_entropy.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "IgspdJn0NdYF"
      },
      "outputs": [],
      "source": [
        "class LabelSmoothedCrossEntropyCriterion(nn.Module):\n",
        "    def __init__(self, smoothing, ignore_index=None, reduce=True):\n",
        "        super().__init__()\n",
        "        self.smoothing = smoothing\n",
        "        self.ignore_index = ignore_index\n",
        "        self.reduce = reduce\n",
        "    \n",
        "    def forward(self, lprobs, target):\n",
        "        if target.dim() == lprobs.dim() - 1:\n",
        "            target = target.unsqueeze(-1)\n",
        "        # nll: Negative log likelihood，the cross-entropy when target is one-hot. following line is same as F.nll_loss\n",
        "        nll_loss = -lprobs.gather(dim=-1, index=target)\n",
        "        #  reserve some probability for other labels. thus when calculating cross-entropy, \n",
        "        # equivalent to summing the log probs of all labels\n",
        "        smooth_loss = -lprobs.sum(dim=-1, keepdim=True)\n",
        "        if self.ignore_index is not None:\n",
        "            pad_mask = target.eq(self.ignore_index)\n",
        "            nll_loss.masked_fill_(pad_mask, 0.0)\n",
        "            smooth_loss.masked_fill_(pad_mask, 0.0)\n",
        "        else:\n",
        "            nll_loss = nll_loss.squeeze(-1)\n",
        "            smooth_loss = smooth_loss.squeeze(-1)\n",
        "        if self.reduce:\n",
        "            nll_loss = nll_loss.sum()\n",
        "            smooth_loss = smooth_loss.sum()\n",
        "        # when calculating cross-entropy, add the loss of other labels\n",
        "        eps_i = self.smoothing / lprobs.size(-1)\n",
        "        loss = (1.0 - self.smoothing) * nll_loss + eps_i * smooth_loss\n",
        "        return loss\n",
        "\n",
        "# generally, 0.1 is good enough\n",
        "criterion = LabelSmoothedCrossEntropyCriterion(\n",
        "    smoothing=0.1,\n",
        "    ignore_index=task.target_dictionary.pad(),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aRalDto2NkJJ"
      },
      "source": [
        "## Optimizer: Adam + lr scheduling\n",
        "Inverse square root scheduling is important to the stability when training Transformer. It's later used on RNN as well.\n",
        "Update the learning rate according to the following equation. Linearly increase the first stage, then decay proportionally to the inverse square root of timestep.\n",
        "$$lrate = d_{\\text{model}}^{-0.5}\\cdot\\min({step\\_num}^{-0.5},{step\\_num}\\cdot{warmup\\_steps}^{-1.5})$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "sS7tQj1ROBYm"
      },
      "outputs": [],
      "source": [
        "def get_rate(d_model, step_num, warmup_step):\n",
        "    # TODO: Change lr from constant to the equation shown above\n",
        "    lr = d_model ** (-0.5) * min(step_num ** (-0.5), step_num * (warmup_step ** (-1.5)))\n",
        "    return lr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "J8hoAjHPNkh3"
      },
      "outputs": [],
      "source": [
        "class NoamOpt:\n",
        "    \"Optim wrapper that implements rate.\"\n",
        "    def __init__(self, model_size, factor, warmup, optimizer):\n",
        "        self.optimizer = optimizer\n",
        "        self._step = 0\n",
        "        self.warmup = warmup\n",
        "        self.factor = factor\n",
        "        self.model_size = model_size\n",
        "        self._rate = 0\n",
        "    \n",
        "    @property\n",
        "    def param_groups(self):\n",
        "        return self.optimizer.param_groups\n",
        "        \n",
        "    def multiply_grads(self, c):\n",
        "        \"\"\"Multiplies grads by a constant *c*.\"\"\"                \n",
        "        for group in self.param_groups:\n",
        "            for p in group['params']:\n",
        "                if p.grad is not None:\n",
        "                    p.grad.data.mul_(c)\n",
        "        \n",
        "    def step(self):\n",
        "        \"Update parameters and rate\"\n",
        "        self._step += 1\n",
        "        rate = self.rate()\n",
        "        for p in self.param_groups:\n",
        "            p['lr'] = rate\n",
        "        self._rate = rate\n",
        "        self.optimizer.step()\n",
        "        \n",
        "    def rate(self, step = None):\n",
        "        \"Implement `lrate` above\"\n",
        "        if step is None:\n",
        "            step = self._step\n",
        "        return 0 if not step else self.factor * get_rate(self.model_size, step, self.warmup)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VFJlkOMONsc6"
      },
      "source": [
        "## Scheduling Visualized"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "A135fwPCNrQs"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": "<Figure size 432x288 with 1 Axes>"
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "optimizer = NoamOpt(\n",
        "    model_size=arch_args.encoder_embed_dim, \n",
        "    factor=config.lr_factor, \n",
        "    warmup=config.lr_warmup, \n",
        "    optimizer=torch.optim.AdamW(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9, weight_decay=0.0001))\n",
        "plt.plot(np.arange(1, 100000), [optimizer.rate(i) for i in range(1, 100000)])\n",
        "plt.legend([f\"{optimizer.model_size}:{optimizer.warmup}\"])\n",
        "None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TOR0g-cVO5ZO"
      },
      "source": [
        "# Training Procedure"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-0ZjbK3O8Iv"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "foal3xM1O404"
      },
      "outputs": [],
      "source": [
        "from fairseq.data import iterators\n",
        "from torch.cuda.amp import GradScaler, autocast\n",
        "\n",
        "def train_one_epoch(epoch_itr, model, task, criterion, optimizer, accum_steps=1):\n",
        "    itr = epoch_itr.next_epoch_itr(shuffle=True)\n",
        "    itr = iterators.GroupedIterator(itr, accum_steps) # gradient accumulation: update every accum_steps samples\n",
        "    \n",
        "    stats = {\"loss\": []}\n",
        "    scaler = GradScaler() # automatic mixed precision (amp) \n",
        "    \n",
        "    model.train()\n",
        "    progress = tqdm.tqdm(itr, desc=f\"train epoch {epoch_itr.epoch}\", leave=False)\n",
        "    for samples in progress:\n",
        "        model.zero_grad()\n",
        "        accum_loss = 0\n",
        "        sample_size = 0\n",
        "        # gradient accumulation: update every accum_steps samples\n",
        "        for i, sample in enumerate(samples):\n",
        "            if i == 1:\n",
        "                # emptying the CUDA cache after the first step can reduce the chance of OOM\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "            sample = utils.move_to_cuda(sample, device=device)\n",
        "            target = sample[\"target\"]\n",
        "            sample_size_i = sample[\"ntokens\"]\n",
        "            sample_size += sample_size_i\n",
        "            \n",
        "            # mixed precision training\n",
        "            with autocast():\n",
        "                net_output = model.forward(**sample[\"net_input\"])\n",
        "                lprobs = F.log_softmax(net_output[0], -1)            \n",
        "                loss = criterion(lprobs.view(-1, lprobs.size(-1)), target.view(-1))\n",
        "                \n",
        "                # logging\n",
        "                accum_loss += loss.item()\n",
        "                # back-prop\n",
        "                scaler.scale(loss).backward()                \n",
        "        \n",
        "        scaler.unscale_(optimizer)\n",
        "        optimizer.multiply_grads(1 / (sample_size or 1.0)) # (sample_size or 1.0) handles the case of a zero gradient\n",
        "        gnorm = nn.utils.clip_grad_norm_(model.parameters(), config.clip_norm) # grad norm clipping prevents gradient exploding\n",
        "        \n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        \n",
        "        # logging\n",
        "        loss_print = accum_loss/sample_size\n",
        "        stats[\"loss\"].append(loss_print)\n",
        "        progress.set_postfix(loss=loss_print)\n",
        "        if config.use_wandb:\n",
        "            wandb.log({\n",
        "                \"train/loss\": loss_print,\n",
        "                \"train/grad_norm\": gnorm.item(),\n",
        "                \"train/lr\": optimizer.rate(),\n",
        "                \"train/sample_size\": sample_size,\n",
        "            })\n",
        "        \n",
        "    loss_print = np.mean(stats[\"loss\"])\n",
        "    logger.info(f\"training loss: {loss_print:.4f}\")\n",
        "    return stats"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gt1lX3DRO_yU"
      },
      "source": [
        "## Validation & Inference\n",
        "To prevent overfitting, validation is required every epoch to validate the performance on unseen data.\n",
        "- the procedure is essensially same as training, with the addition of inference step\n",
        "- after validation we can save the model weights\n",
        "\n",
        "Validation loss alone cannot describe the actual performance of the model\n",
        "- Directly produce translation hypotheses based on current model, then calculate BLEU with the reference translation\n",
        "- We can also manually examine the hypotheses' quality\n",
        "- We use fairseq's sequence generator for beam search to generate translation hypotheses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "2og80HYQPAKq"
      },
      "outputs": [],
      "source": [
        "# fairseq's beam search generator\n",
        "# given model and input seqeunce, produce translation hypotheses by beam search\n",
        "sequence_generator = task.build_generator([model], config)\n",
        "\n",
        "def decode(toks, dictionary):\n",
        "    # convert from Tensor to human readable sentence\n",
        "    s = dictionary.string(\n",
        "        toks.int().cpu(),\n",
        "        config.post_process,\n",
        "    )\n",
        "    return s if s else \"<unk>\"\n",
        "\n",
        "def inference_step(sample, model):\n",
        "    gen_out = sequence_generator.generate([model], sample)\n",
        "    srcs = []\n",
        "    hyps = []\n",
        "    refs = []\n",
        "    for i in range(len(gen_out)):\n",
        "        # for each sample, collect the input, hypothesis and reference, later be used to calculate BLEU\n",
        "        srcs.append(decode(\n",
        "            utils.strip_pad(sample[\"net_input\"][\"src_tokens\"][i], task.source_dictionary.pad()), \n",
        "            task.source_dictionary,\n",
        "        ))\n",
        "        hyps.append(decode(\n",
        "            gen_out[i][0][\"tokens\"], # 0 indicates using the top hypothesis in beam\n",
        "            task.target_dictionary,\n",
        "        ))\n",
        "        refs.append(decode(\n",
        "            utils.strip_pad(sample[\"target\"][i], task.target_dictionary.pad()), \n",
        "            task.target_dictionary,\n",
        "        ))\n",
        "    return srcs, hyps, refs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "y1o7LeDkPDsd"
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "import sacrebleu\n",
        "\n",
        "def validate(model, task, criterion, log_to_wandb=True):\n",
        "    logger.info('begin validation')\n",
        "    itr = load_data_iterator(task, \"valid\", 1, config.max_tokens, config.num_workers).next_epoch_itr(shuffle=False)\n",
        "    \n",
        "    stats = {\"loss\":[], \"bleu\": 0, \"srcs\":[], \"hyps\":[], \"refs\":[]}\n",
        "    srcs = []\n",
        "    hyps = []\n",
        "    refs = []\n",
        "    \n",
        "    model.eval()\n",
        "    progress = tqdm.tqdm(itr, desc=f\"validation\", leave=False)\n",
        "    with torch.no_grad():\n",
        "        for i, sample in enumerate(progress):\n",
        "            # validation loss\n",
        "            sample = utils.move_to_cuda(sample, device=device)\n",
        "            net_output = model.forward(**sample[\"net_input\"])\n",
        "\n",
        "            lprobs = F.log_softmax(net_output[0], -1)\n",
        "            target = sample[\"target\"]\n",
        "            sample_size = sample[\"ntokens\"]\n",
        "            loss = criterion(lprobs.view(-1, lprobs.size(-1)), target.view(-1)) / sample_size\n",
        "            progress.set_postfix(valid_loss=loss.item())\n",
        "            stats[\"loss\"].append(loss)\n",
        "            \n",
        "            # do inference\n",
        "            s, h, r = inference_step(sample, model)\n",
        "            srcs.extend(s)\n",
        "            hyps.extend(h)\n",
        "            refs.extend(r)\n",
        "            \n",
        "    tok = 'zh' if task.cfg.target_lang == 'zh' else '13a'\n",
        "    stats[\"loss\"] = torch.stack(stats[\"loss\"]).mean().item()\n",
        "    stats[\"bleu\"] = sacrebleu.corpus_bleu(hyps, [refs], tokenize=tok) # 計算BLEU score\n",
        "    stats[\"srcs\"] = srcs\n",
        "    stats[\"hyps\"] = hyps\n",
        "    stats[\"refs\"] = refs\n",
        "    \n",
        "    if config.use_wandb and log_to_wandb:\n",
        "        wandb.log({\n",
        "            \"valid/loss\": stats[\"loss\"],\n",
        "            \"valid/bleu\": stats[\"bleu\"].score,\n",
        "        }, commit=False)\n",
        "    \n",
        "    showid = np.random.randint(len(hyps))\n",
        "    logger.info(\"example source: \" + srcs[showid])\n",
        "    logger.info(\"example hypothesis: \" + hyps[showid])\n",
        "    logger.info(\"example reference: \" + refs[showid])\n",
        "    \n",
        "    # show bleu results\n",
        "    logger.info(f\"validation loss:\\t{stats['loss']:.4f}\")\n",
        "    logger.info(stats[\"bleu\"].format())\n",
        "    return stats"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1sRF6nd4PGEE"
      },
      "source": [
        "# Save and Load Model Weights\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "edBuLlkuPGr9"
      },
      "outputs": [],
      "source": [
        "def validate_and_save(model, task, criterion, optimizer, epoch, save=True):   \n",
        "    stats = validate(model, task, criterion)\n",
        "    bleu = stats['bleu']\n",
        "    loss = stats['loss']\n",
        "    if save:\n",
        "        # save epoch checkpoints\n",
        "        savedir = Path(config.savedir).absolute()\n",
        "        savedir.mkdir(parents=True, exist_ok=True)\n",
        "        \n",
        "        check = {\n",
        "            \"model\": model.state_dict(),\n",
        "            \"stats\": {\"bleu\": bleu.score, \"loss\": loss},\n",
        "            \"optim\": {\"step\": optimizer._step}\n",
        "        }\n",
        "        torch.save(check, savedir/f\"checkpoint{epoch}.pt\")\n",
        "        shutil.copy(savedir/f\"checkpoint{epoch}.pt\", savedir/f\"checkpoint_last.pt\")\n",
        "        logger.info(f\"saved epoch checkpoint: {savedir}/checkpoint{epoch}.pt\")\n",
        "    \n",
        "        # save epoch samples\n",
        "        with open(savedir/f\"samples{epoch}.{config.source_lang}-{config.target_lang}.txt\", \"w\") as f:\n",
        "            for s, h in zip(stats[\"srcs\"], stats[\"hyps\"]):\n",
        "                f.write(f\"{s}\\t{h}\\n\")\n",
        "\n",
        "        # get best valid bleu    \n",
        "        if getattr(validate_and_save, \"best_bleu\", 0) < bleu.score:\n",
        "            validate_and_save.best_bleu = bleu.score\n",
        "            torch.save(check, savedir/f\"checkpoint_best.pt\")\n",
        "            \n",
        "        del_file = savedir / f\"checkpoint{epoch - config.keep_last_epochs}.pt\"\n",
        "        if del_file.exists():\n",
        "            del_file.unlink()\n",
        "    \n",
        "    return stats\n",
        "\n",
        "def try_load_checkpoint(model, optimizer=None, name=None):\n",
        "    name = name if name else \"checkpoint_last.pt\"\n",
        "    checkpath = Path(config.savedir)/name\n",
        "    if checkpath.exists():\n",
        "        check = torch.load(checkpath)\n",
        "        model.load_state_dict(check[\"model\"])\n",
        "        stats = check[\"stats\"]\n",
        "        step = \"unknown\"\n",
        "        if optimizer != None:\n",
        "            optimizer._step = step = check[\"optim\"][\"step\"]\n",
        "        logger.info(f\"loaded checkpoint {checkpath}: step={step} loss={stats['loss']} bleu={stats['bleu']}\")\n",
        "    else:\n",
        "        logger.info(f\"no checkpoints found at {checkpath}!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KyIFpibfPJ5u"
      },
      "source": [
        "# Main\n",
        "## Training loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "hu7RZbCUPKQr"
      },
      "outputs": [],
      "source": [
        "model = model.to(device=device)\n",
        "criterion = criterion.to(device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "5xxlJxU2PeAo"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-03 12:21:48 | INFO | hw5.seq2seq | task: TranslationTask\n",
            "2022-04-03 12:21:48 | INFO | hw5.seq2seq | encoder: TransformerEncoder\n",
            "2022-04-03 12:21:48 | INFO | hw5.seq2seq | decoder: TransformerDecoder\n",
            "2022-04-03 12:21:48 | INFO | hw5.seq2seq | criterion: LabelSmoothedCrossEntropyCriterion\n",
            "2022-04-03 12:21:48 | INFO | hw5.seq2seq | optimizer: NoamOpt\n",
            "2022-04-03 12:21:48 | INFO | hw5.seq2seq | num. model params: 52,332,544 (num. trained: 52,332,544)\n",
            "2022-04-03 12:21:48 | INFO | hw5.seq2seq | max tokens per batch = 8192, accumulate steps = 2\n"
          ]
        }
      ],
      "source": [
        "logger.info(\"task: {}\".format(task.__class__.__name__))\n",
        "logger.info(\"encoder: {}\".format(model.encoder.__class__.__name__))\n",
        "logger.info(\"decoder: {}\".format(model.decoder.__class__.__name__))\n",
        "logger.info(\"criterion: {}\".format(criterion.__class__.__name__))\n",
        "logger.info(\"optimizer: {}\".format(optimizer.__class__.__name__))\n",
        "logger.info(\n",
        "    \"num. model params: {:,} (num. trained: {:,})\".format(\n",
        "        sum(p.numel() for p in model.parameters()),\n",
        "        sum(p.numel() for p in model.parameters() if p.requires_grad),\n",
        "    )\n",
        ")\n",
        "logger.info(f\"max tokens per batch = {config.max_tokens}, accumulate steps = {config.accum_steps}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "MSPRqpQUPfaX"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-03 12:21:51 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[326674]\n",
            "2022-04-03 12:21:51 | INFO | hw5.seq2seq | no checkpoints found at checkpoints/rnn/checkpoint_last.pt!\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c6607460d3a24c8dbefde379caf57f36",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": "train epoch 1:   0%|          | 0/791 [00:00<?, ?it/s]"
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-03 12:34:16 | INFO | hw5.seq2seq | training loss: 6.5241\n",
            "2022-04-03 12:34:16 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1ec308862227433e8330d324afc6296b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": "validation:   0%|          | 0/22 [00:00<?, ?it/s]"
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/haoyu/.local/lib/python3.8/site-packages/fairseq/search.py:140: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  beams_buf = indices_buf // vocab_size\n",
            "/home/haoyu/.local/lib/python3.8/site-packages/fairseq/sequence_generator.py:657: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  unfin_idx = idx // beam_size\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-03 12:34:52 | INFO | hw5.seq2seq | example source: and i would suggest that this has something to do with the internet . it's not a fluke .\n",
            "2022-04-03 12:34:52 | INFO | hw5.seq2seq | example hypothesis: 我想這張照片 , 把這張張圖像這樣 。\n",
            "2022-04-03 12:34:52 | INFO | hw5.seq2seq | example reference: 我認為這必須歸功於網路的力量 。 這不是僥倖 ,\n",
            "2022-04-03 12:34:52 | INFO | hw5.seq2seq | validation loss:\t5.2304\n",
            "2022-04-03 12:34:52 | INFO | hw5.seq2seq | BLEU = 4.76 26.8/8.0/2.7/1.0 (BP = 0.970 ratio = 0.970 hyp_len = 108508 ref_len = 111811)\n",
            "2022-04-03 12:34:53 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/haoyu/Desktop/2022ML/hw5/checkpoints/rnn/checkpoint1.pt\n",
            "2022-04-03 12:34:53 | INFO | hw5.seq2seq | end of epoch 1\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f41a6b918f69489c84ae55462c4f40af",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": "train epoch 2:   0%|          | 0/791 [00:00<?, ?it/s]"
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-03 12:47:19 | INFO | hw5.seq2seq | training loss: 4.7858\n",
            "2022-04-03 12:47:19 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "16bb6b3a9753472b89221eeeca56a93b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": "validation:   0%|          | 0/22 [00:00<?, ?it/s]"
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-03 12:47:48 | INFO | hw5.seq2seq | example source: and it's even believed to have been used in building the egyptian pyramids .\n",
            "2022-04-03 12:47:48 | INFO | hw5.seq2seq | example hypothesis: 即使 , 相信 , 建立在埃及的博士學位上 。\n",
            "2022-04-03 12:47:48 | INFO | hw5.seq2seq | example reference: 大家也相信埃及金字塔和這數字有關\n",
            "2022-04-03 12:47:48 | INFO | hw5.seq2seq | validation loss:\t4.3222\n",
            "2022-04-03 12:47:48 | INFO | hw5.seq2seq | BLEU = 13.29 44.2/20.0/9.9/5.1 (BP = 0.916 ratio = 0.920 hyp_len = 102829 ref_len = 111811)\n",
            "2022-04-03 12:47:49 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/haoyu/Desktop/2022ML/hw5/checkpoints/rnn/checkpoint2.pt\n",
            "2022-04-03 12:47:50 | INFO | hw5.seq2seq | end of epoch 2\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5ff63f622b2a452ea9d7e6d06a5015e6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": "train epoch 3:   0%|          | 0/791 [00:00<?, ?it/s]"
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-03 13:00:13 | INFO | hw5.seq2seq | training loss: 4.1343\n",
            "2022-04-03 13:00:13 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c5d5b6b7f3974749a32657dc54754aed",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": "validation:   0%|          | 0/22 [00:00<?, ?it/s]"
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-03 13:00:41 | INFO | hw5.seq2seq | example source: we've got to do everything we can to push our governments and companies to do a better job of protecting their rights .\n",
            "2022-04-03 13:00:41 | INFO | hw5.seq2seq | example hypothesis: 我們必須做的一切 , 來推出我們的政府和公司 , 來保護他們的權利 。\n",
            "2022-04-03 13:00:41 | INFO | hw5.seq2seq | example reference: 我們必須盡我們所能去促使我們的政府和企業能更保護這些人的權利 。\n",
            "2022-04-03 13:00:41 | INFO | hw5.seq2seq | validation loss:\t3.9206\n",
            "2022-04-03 13:00:41 | INFO | hw5.seq2seq | BLEU = 17.51 52.2/26.5/14.3/8.1 (BP = 0.876 ratio = 0.883 hyp_len = 98733 ref_len = 111811)\n",
            "2022-04-03 13:00:42 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/haoyu/Desktop/2022ML/hw5/checkpoints/rnn/checkpoint3.pt\n",
            "2022-04-03 13:00:42 | INFO | hw5.seq2seq | end of epoch 3\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f7a61c6be1e8425db16544a3fd4ebf9b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": "train epoch 4:   0%|          | 0/791 [00:00<?, ?it/s]"
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-03 13:13:07 | INFO | hw5.seq2seq | training loss: 3.8182\n",
            "2022-04-03 13:13:07 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6afad057e23948b0b414fc77ecfe3ec2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": "validation:   0%|          | 0/22 [00:00<?, ?it/s]"
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-03 13:13:36 | INFO | hw5.seq2seq | example source: there are some newer technologies for delivering that have happened that are actually quite exciting as well .\n",
            "2022-04-03 13:13:36 | INFO | hw5.seq2seq | example hypothesis: 有些更新的科技 , 可以傳遞出去 , 真的很令人興奮 。\n",
            "2022-04-03 13:13:36 | INFO | hw5.seq2seq | example reference: 目前有較新的傳輸技術其成果也相當令人振奮\n",
            "2022-04-03 13:13:36 | INFO | hw5.seq2seq | validation loss:\t3.7228\n",
            "2022-04-03 13:13:36 | INFO | hw5.seq2seq | BLEU = 20.07 54.3/28.5/15.9/9.2 (BP = 0.921 ratio = 0.924 hyp_len = 103316 ref_len = 111811)\n",
            "2022-04-03 13:13:36 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/haoyu/Desktop/2022ML/hw5/checkpoints/rnn/checkpoint4.pt\n",
            "2022-04-03 13:13:37 | INFO | hw5.seq2seq | end of epoch 4\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "04693eaad1f54025bfe663745580dbf4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": "train epoch 5:   0%|          | 0/791 [00:00<?, ?it/s]"
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-03 13:25:58 | INFO | hw5.seq2seq | training loss: 3.6497\n",
            "2022-04-03 13:25:58 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9bc7b27c908d41ec85654d446f18cff5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": "validation:   0%|          | 0/22 [00:00<?, ?it/s]"
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-03 13:26:23 | INFO | hw5.seq2seq | example source: only the best ones can survive .\n",
            "2022-04-03 13:26:23 | INFO | hw5.seq2seq | example hypothesis: 只有最好的人能存活下來 。\n",
            "2022-04-03 13:26:23 | INFO | hw5.seq2seq | example reference: 但只有最好的能夠勝出\n",
            "2022-04-03 13:26:23 | INFO | hw5.seq2seq | validation loss:\t3.6252\n",
            "2022-04-03 13:26:23 | INFO | hw5.seq2seq | BLEU = 19.60 57.0/30.4/17.1/10.1 (BP = 0.839 ratio = 0.851 hyp_len = 95139 ref_len = 111811)\n",
            "2022-04-03 13:26:24 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/haoyu/Desktop/2022ML/hw5/checkpoints/rnn/checkpoint5.pt\n",
            "2022-04-03 13:26:24 | INFO | hw5.seq2seq | end of epoch 5\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2b20b7418aec487c897ce903e10ce9a3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": "train epoch 6:   0%|          | 0/791 [00:00<?, ?it/s]"
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-03 13:38:46 | INFO | hw5.seq2seq | training loss: 3.5086\n",
            "2022-04-03 13:38:46 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "79fc43b0cc8c4b57800bde04f9f87a5c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": "validation:   0%|          | 0/22 [00:00<?, ?it/s]"
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-03 13:39:12 | INFO | hw5.seq2seq | example source: and then for a real beginner , you would go down even further and elaborate more say , \" open the oven , preheat , wait for the light to go out , open the door , don't leave it open too long , put the penguin in and shut the door . . . \" whatever .\n",
            "2022-04-03 13:39:12 | INFO | hw5.seq2seq | example hypothesis: 接著 , 真正的開頭 , 你會更進一步、更精密地說: 「 打開烤箱 , 等著燈 , 打開門 , 不要把它打開 , 把企鵝打開 , 把門打開 , 把企鵝打開 , 打開 。 」\n",
            "2022-04-03 13:39:12 | INFO | hw5.seq2seq | example reference: 對於一個新手 , 你會繼續下去並且解釋更多 。 \" 打開烤爐 , 預熱 , 等到燈亮打開門 , 不要打開太久把企鵝放進去 , 然後關門 \" 等等\n",
            "2022-04-03 13:39:12 | INFO | hw5.seq2seq | validation loss:\t3.4813\n",
            "2022-04-03 13:39:12 | INFO | hw5.seq2seq | BLEU = 22.29 57.6/31.5/18.1/10.9 (BP = 0.910 ratio = 0.914 hyp_len = 102191 ref_len = 111811)\n",
            "2022-04-03 13:39:13 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/haoyu/Desktop/2022ML/hw5/checkpoints/rnn/checkpoint6.pt\n",
            "2022-04-03 13:39:14 | INFO | hw5.seq2seq | end of epoch 6\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d35495e5b83047edbf18e417183843ea",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": "train epoch 7:   0%|          | 0/791 [00:00<?, ?it/s]"
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-03 13:51:37 | INFO | hw5.seq2seq | training loss: 3.3483\n",
            "2022-04-03 13:51:37 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6d2c3bdcfc4949e6bca8e865b14a1799",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": "validation:   0%|          | 0/22 [00:00<?, ?it/s]"
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-03 13:52:04 | INFO | hw5.seq2seq | example source: rl: i think terrorism is still number one .\n",
            "2022-04-03 13:52:04 | INFO | hw5.seq2seq | example hypothesis: 理查德:我認為恐怖主義仍然是第一名 。\n",
            "2022-04-03 13:52:04 | INFO | hw5.seq2seq | example reference: 理查德:我認為恐怖主義仍然是第一 。\n",
            "2022-04-03 13:52:04 | INFO | hw5.seq2seq | validation loss:\t3.4107\n",
            "2022-04-03 13:52:04 | INFO | hw5.seq2seq | BLEU = 23.19 57.2/31.6/18.5/11.3 (BP = 0.936 ratio = 0.938 hyp_len = 104880 ref_len = 111811)\n",
            "2022-04-03 13:52:05 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/haoyu/Desktop/2022ML/hw5/checkpoints/rnn/checkpoint7.pt\n",
            "2022-04-03 13:52:06 | INFO | hw5.seq2seq | end of epoch 7\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c5522a1884a24bf3acdfeb68493237c0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": "train epoch 8:   0%|          | 0/791 [00:00<?, ?it/s]"
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-03 14:04:27 | INFO | hw5.seq2seq | training loss: 3.2265\n",
            "2022-04-03 14:04:27 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5151752dee0d4f3081305706198ce198",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": "validation:   0%|          | 0/22 [00:00<?, ?it/s]"
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-03 14:04:54 | INFO | hw5.seq2seq | example source: so obviously , i had to crashland a grandma on america chavez , and not just any grandma a big , strong , luchador grandma , one that loved her enough to take her to the ancestral plain , where america chavez could see the history of her people play out in the skies above .\n",
            "2022-04-03 14:04:54 | INFO | hw5.seq2seq | example hypothesis: 很顯然 , 我得撞上美國大師查韋茲 , 不僅僅是一位堅強、富裕的奶奶 , 她喜歡她帶她到祖宗平原上 , 美國查韋茲看到她的人們在天空中玩耍的歷史 。\n",
            "2022-04-03 14:04:54 | INFO | hw5.seq2seq | example reference: 很明顯 , 我得要丟一個祖母給艾美莉卡查韋斯 , 且不是任何祖母是個高大強壯會摔角的祖母 , 這個祖母非常愛她 , 因此帶她到祖先的平原 , 在那裡 , 艾美莉卡查韋斯能夠看到她的族人的歷史 , 在天空上呈現出來 。\n",
            "2022-04-03 14:04:54 | INFO | hw5.seq2seq | validation loss:\t3.3495\n",
            "2022-04-03 14:04:54 | INFO | hw5.seq2seq | BLEU = 23.27 60.2/33.8/19.9/12.3 (BP = 0.876 ratio = 0.883 hyp_len = 98750 ref_len = 111811)\n",
            "2022-04-03 14:04:54 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/haoyu/Desktop/2022ML/hw5/checkpoints/rnn/checkpoint8.pt\n",
            "2022-04-03 14:04:55 | INFO | hw5.seq2seq | end of epoch 8\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0c1b044e39004b36ba0771237e895a63",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": "train epoch 9:   0%|          | 0/791 [00:00<?, ?it/s]"
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-03 14:17:16 | INFO | hw5.seq2seq | training loss: 3.1342\n",
            "2022-04-03 14:17:16 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "089b3e89df3d4f3cb667cba0bf46eedc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": "validation:   0%|          | 0/22 [00:00<?, ?it/s]"
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-03 14:17:44 | INFO | hw5.seq2seq | example source: we've come to the end of the talk , and i will reveal what is in the bag , and it is the muse , and it is the things that transform in our lives , that are wonderful and stay with us .\n",
            "2022-04-03 14:17:44 | INFO | hw5.seq2seq | example hypothesis: 我們來到演講的尾聲 , 我會揭露在袋子裡的東西 , 它是博物館 , 它是改變我們生活的事物 , 美好並與我們同在 。\n",
            "2022-04-03 14:17:44 | INFO | hw5.seq2seq | example reference: 這場演說到了尾聲而我將揭露隱藏的秘密 , 就是謬思靈感的來源就是這玩意改變了我們的生活美好且一直與我們在一起\n",
            "2022-04-03 14:17:44 | INFO | hw5.seq2seq | validation loss:\t3.2951\n",
            "2022-04-03 14:17:44 | INFO | hw5.seq2seq | BLEU = 25.00 58.5/33.0/19.6/12.2 (BP = 0.958 ratio = 0.959 hyp_len = 107256 ref_len = 111811)\n",
            "2022-04-03 14:17:45 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/haoyu/Desktop/2022ML/hw5/checkpoints/rnn/checkpoint9.pt\n",
            "2022-04-03 14:17:46 | INFO | hw5.seq2seq | end of epoch 9\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "19860417e4c64065b706c22d894371fa",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": "train epoch 10:   0%|          | 0/791 [00:00<?, ?it/s]"
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-03 14:30:08 | INFO | hw5.seq2seq | training loss: 3.0510\n",
            "2022-04-03 14:30:08 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bc244adab7a7497d88ecbe9435580158",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": "validation:   0%|          | 0/22 [00:00<?, ?it/s]"
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-03 14:30:37 | INFO | hw5.seq2seq | example source: until biofabrication is better understood , it is clear that , initially at least , more people would be willing to wear novel materials than would be willing to eat novel foods , no matter how delicious .\n",
            "2022-04-03 14:30:37 | INFO | hw5.seq2seq | example hypothesis: 除非生物製造比起願意吃新鮮食物 , 很明顯地 , 一開始 , 很明顯地 , 有些人會願意去除新鮮食物 , 不論多麼美味 。\n",
            "2022-04-03 14:30:37 | INFO | hw5.seq2seq | example reference: 在更暸解生物製造之前顯然 , 至少剛開始時較多人會願意穿戴新奇的材質較少人會願意吃新奇的食物無論有多好吃\n",
            "2022-04-03 14:30:37 | INFO | hw5.seq2seq | validation loss:\t3.2639\n",
            "2022-04-03 14:30:37 | INFO | hw5.seq2seq | BLEU = 25.21 58.2/32.8/19.5/12.2 (BP = 0.971 ratio = 0.972 hyp_len = 108648 ref_len = 111811)\n",
            "2022-04-03 14:30:37 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/haoyu/Desktop/2022ML/hw5/checkpoints/rnn/checkpoint10.pt\n",
            "2022-04-03 14:30:38 | INFO | hw5.seq2seq | end of epoch 10\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "55a1b48838b842ef8fa17d7cb9e0d51a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": "train epoch 11:   0%|          | 0/791 [00:00<?, ?it/s]"
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-03 14:42:58 | INFO | hw5.seq2seq | training loss: 2.9852\n",
            "2022-04-03 14:42:58 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "724d0e6a839e46349f67a1461bcbd38b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": "validation:   0%|          | 0/22 [00:00<?, ?it/s]"
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-03 14:43:25 | INFO | hw5.seq2seq | example source: no ? yes ? doesn't matter , it worked .\n",
            "2022-04-03 14:43:25 | INFO | hw5.seq2seq | example hypothesis: 不對嗎 ? 沒關係 , 沒用 。\n",
            "2022-04-03 14:43:25 | INFO | hw5.seq2seq | example reference: 有 ? 沒有 ? 沒關係 , 反正能用就好 。\n",
            "2022-04-03 14:43:25 | INFO | hw5.seq2seq | validation loss:\t3.2518\n",
            "2022-04-03 14:43:25 | INFO | hw5.seq2seq | BLEU = 25.32 59.5/34.0/20.4/12.8 (BP = 0.941 ratio = 0.942 hyp_len = 105354 ref_len = 111811)\n",
            "2022-04-03 14:43:26 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/haoyu/Desktop/2022ML/hw5/checkpoints/rnn/checkpoint11.pt\n",
            "2022-04-03 14:43:26 | INFO | hw5.seq2seq | end of epoch 11\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "513adca0dca941529cfdb5d8811e72b5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": "train epoch 12:   0%|          | 0/791 [00:00<?, ?it/s]"
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-03 14:55:49 | INFO | hw5.seq2seq | training loss: 2.9294\n",
            "2022-04-03 14:55:49 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c2583e1352f24d79a81aa5838a13c217",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": "validation:   0%|          | 0/22 [00:00<?, ?it/s]"
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-03 14:56:17 | INFO | hw5.seq2seq | example source: how many followers have i gained ?\n",
            "2022-04-03 14:56:17 | INFO | hw5.seq2seq | example hypothesis: 我有多少追隨者 ?\n",
            "2022-04-03 14:56:17 | INFO | hw5.seq2seq | example reference: 我有多少新的追蹤者 ?\n",
            "2022-04-03 14:56:17 | INFO | hw5.seq2seq | validation loss:\t3.2550\n",
            "2022-04-03 14:56:17 | INFO | hw5.seq2seq | BLEU = 25.94 58.8/33.7/20.3/12.8 (BP = 0.969 ratio = 0.969 hyp_len = 108370 ref_len = 111811)\n",
            "2022-04-03 14:56:18 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/haoyu/Desktop/2022ML/hw5/checkpoints/rnn/checkpoint12.pt\n",
            "2022-04-03 14:56:19 | INFO | hw5.seq2seq | end of epoch 12\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "88185b0cc04a49f89b24b50be80518d0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": "train epoch 13:   0%|          | 0/791 [00:00<?, ?it/s]"
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-03 15:08:40 | INFO | hw5.seq2seq | training loss: 2.8766\n",
            "2022-04-03 15:08:40 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7fc206ea434d4460be412ec4e5c9b010",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": "validation:   0%|          | 0/22 [00:00<?, ?it/s]"
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-03 15:09:09 | INFO | hw5.seq2seq | example source: imagine a patch that you wear on the skin that would detect in your sweat when you're infected with malaria and change color .\n",
            "2022-04-03 15:09:09 | INFO | hw5.seq2seq | example hypothesis: 想像一下 , 當你感染瘧疾時 , 穿著皮膚上的貼片 , 會在你的汗水中偵測到 , 且會變色 。\n",
            "2022-04-03 15:09:09 | INFO | hw5.seq2seq | example reference: 想像一下貼在皮膚上的貼片可以偵測你的汗水當你感染了瘧疾貼片的顏色就會改變\n",
            "2022-04-03 15:09:09 | INFO | hw5.seq2seq | validation loss:\t3.2587\n",
            "2022-04-03 15:09:09 | INFO | hw5.seq2seq | BLEU = 25.75 59.0/33.8/20.3/12.8 (BP = 0.960 ratio = 0.961 hyp_len = 107429 ref_len = 111811)\n",
            "2022-04-03 15:09:09 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/haoyu/Desktop/2022ML/hw5/checkpoints/rnn/checkpoint13.pt\n",
            "2022-04-03 15:09:09 | INFO | hw5.seq2seq | end of epoch 13\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "058d83eacabe4218a77b591c7186d3ba",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": "train epoch 14:   0%|          | 0/791 [00:00<?, ?it/s]"
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-03 15:21:31 | INFO | hw5.seq2seq | training loss: 2.8295\n",
            "2022-04-03 15:21:31 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c2440a3c18614c85bd7b9c99db161c35",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": "validation:   0%|          | 0/22 [00:00<?, ?it/s]"
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-03 15:21:57 | INFO | hw5.seq2seq | example source: mothers have gotten the message and the vast majority of mothers intend to breastfeed , but many do not reach their breastfeeding goals .\n",
            "2022-04-03 15:21:57 | INFO | hw5.seq2seq | example hypothesis: 母親們得到了這個訊息 , 絕大部分的母親都想要哺乳 , 但許多人無法達到她們的母乳餵養目標 。\n",
            "2022-04-03 15:21:57 | INFO | hw5.seq2seq | example reference: 很多母親已經明白和接受這信息 , 而大多數的母親也決定用母乳撫育嬰兒 , 但還是很多人沒有達到她們的哺乳目標 。\n",
            "2022-04-03 15:21:57 | INFO | hw5.seq2seq | validation loss:\t3.2398\n",
            "2022-04-03 15:21:57 | INFO | hw5.seq2seq | BLEU = 25.58 60.1/34.5/20.7/13.1 (BP = 0.934 ratio = 0.936 hyp_len = 104685 ref_len = 111811)\n",
            "2022-04-03 15:21:58 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/haoyu/Desktop/2022ML/hw5/checkpoints/rnn/checkpoint14.pt\n",
            "2022-04-03 15:21:58 | INFO | hw5.seq2seq | end of epoch 14\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "45ea17522cd34ec4a11a87261ab357d7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": "train epoch 15:   0%|          | 0/791 [00:00<?, ?it/s]"
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-03 15:34:20 | INFO | hw5.seq2seq | training loss: 2.7889\n",
            "2022-04-03 15:34:20 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "70e4b18a38ad4feca59aeb6365dd2521",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": "validation:   0%|          | 0/22 [00:00<?, ?it/s]"
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-03 15:34:46 | INFO | hw5.seq2seq | example source: they saw everything .\n",
            "2022-04-03 15:34:46 | INFO | hw5.seq2seq | example hypothesis: 他們看到了一切 。\n",
            "2022-04-03 15:34:46 | INFO | hw5.seq2seq | example reference: 他們看到了一切 。\n",
            "2022-04-03 15:34:46 | INFO | hw5.seq2seq | validation loss:\t3.2578\n",
            "2022-04-03 15:34:46 | INFO | hw5.seq2seq | BLEU = 25.63 60.7/35.0/21.2/13.4 (BP = 0.919 ratio = 0.922 hyp_len = 103127 ref_len = 111811)\n",
            "2022-04-03 15:34:47 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/haoyu/Desktop/2022ML/hw5/checkpoints/rnn/checkpoint15.pt\n",
            "2022-04-03 15:34:47 | INFO | hw5.seq2seq | end of epoch 15\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3d6523efa46b44fb9e4c47d7f35a1729",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": "train epoch 16:   0%|          | 0/791 [00:00<?, ?it/s]"
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-03 15:47:09 | INFO | hw5.seq2seq | training loss: 2.7526\n",
            "2022-04-03 15:47:09 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d50a61ad0fb8440f807d630a7eb08873",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": "validation:   0%|          | 0/22 [00:00<?, ?it/s]"
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-03 15:47:37 | INFO | hw5.seq2seq | example source: from the moment that our ancestors , perhaps two and a half million years ago or so , began imitating , there was a new copying process .\n",
            "2022-04-03 15:47:37 | INFO | hw5.seq2seq | example hypothesis: 從我們的祖先開始 , 大約兩百五十萬年前開始模仿 , 開始有新的複製過程 。\n",
            "2022-04-03 15:47:37 | INFO | hw5.seq2seq | example reference: 自從我們祖先大約250萬年前 , 開始模仿 , 就有一個新的複製過程 。\n",
            "2022-04-03 15:47:37 | INFO | hw5.seq2seq | validation loss:\t3.2408\n",
            "2022-04-03 15:47:37 | INFO | hw5.seq2seq | BLEU = 25.94 59.5/34.1/20.6/13.0 (BP = 0.956 ratio = 0.957 hyp_len = 106972 ref_len = 111811)\n",
            "2022-04-03 15:47:38 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/haoyu/Desktop/2022ML/hw5/checkpoints/rnn/checkpoint16.pt\n",
            "2022-04-03 15:47:39 | INFO | hw5.seq2seq | end of epoch 16\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3552f9690c034d7b9362f9f8944b557d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": "train epoch 17:   0%|          | 0/791 [00:00<?, ?it/s]"
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-03 16:00:00 | INFO | hw5.seq2seq | training loss: 2.7144\n",
            "2022-04-03 16:00:00 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f50cc726092b4f4b80a62c1971a469c4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": "validation:   0%|          | 0/22 [00:00<?, ?it/s]"
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-03 16:00:27 | INFO | hw5.seq2seq | example source: i mean 100 percent death rate is pretty severe .\n",
            "2022-04-03 16:00:27 | INFO | hw5.seq2seq | example hypothesis: 我的意思是100%的死亡率相當嚴重 。\n",
            "2022-04-03 16:00:27 | INFO | hw5.seq2seq | example reference: 我的意思是說100%死亡率相當嚴重呢\n",
            "2022-04-03 16:00:27 | INFO | hw5.seq2seq | validation loss:\t3.2593\n",
            "2022-04-03 16:00:27 | INFO | hw5.seq2seq | BLEU = 26.04 59.5/34.3/20.7/13.1 (BP = 0.954 ratio = 0.955 hyp_len = 106812 ref_len = 111811)\n",
            "2022-04-03 16:00:28 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/haoyu/Desktop/2022ML/hw5/checkpoints/rnn/checkpoint17.pt\n",
            "2022-04-03 16:00:35 | INFO | hw5.seq2seq | end of epoch 17\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "95d2a20a36a945e6844369f9bd62325c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": "train epoch 18:   0%|          | 0/791 [00:00<?, ?it/s]"
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-03 16:12:59 | INFO | hw5.seq2seq | training loss: 2.6824\n",
            "2022-04-03 16:12:59 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2a8d2379219743369508bbb6edcc5aa3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": "validation:   0%|          | 0/22 [00:00<?, ?it/s]"
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-03 16:13:28 | INFO | hw5.seq2seq | example source: howard is a man of a certain degree of intellectual standards .\n",
            "2022-04-03 16:13:28 | INFO | hw5.seq2seq | example hypothesis: 霍華是個有特定知識標準的人 。\n",
            "2022-04-03 16:13:28 | INFO | hw5.seq2seq | example reference: 默斯克韋茲是個秉持某程度的智慧標準的人 。\n",
            "2022-04-03 16:13:28 | INFO | hw5.seq2seq | validation loss:\t3.2539\n",
            "2022-04-03 16:13:28 | INFO | hw5.seq2seq | BLEU = 26.02 58.3/33.2/20.0/12.7 (BP = 0.983 ratio = 0.983 hyp_len = 109894 ref_len = 111811)\n",
            "2022-04-03 16:13:29 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/haoyu/Desktop/2022ML/hw5/checkpoints/rnn/checkpoint18.pt\n",
            "2022-04-03 16:13:29 | INFO | hw5.seq2seq | end of epoch 18\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4b08642727124e5daacef9351ef26f21",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": "train epoch 19:   0%|          | 0/791 [00:00<?, ?it/s]"
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-03 16:25:52 | INFO | hw5.seq2seq | training loss: 2.6521\n",
            "2022-04-03 16:25:52 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e6a5322370cf4346a864665915f28b9c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": "validation:   0%|          | 0/22 [00:00<?, ?it/s]"
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-03 16:26:20 | INFO | hw5.seq2seq | example source: we're going to offer it as a donation to this poor country .\n",
            "2022-04-03 16:26:20 | INFO | hw5.seq2seq | example hypothesis: 我們會把它捐給這個貧窮的國家 。\n",
            "2022-04-03 16:26:20 | INFO | hw5.seq2seq | example reference: 我們要把這些捐給窮國\n",
            "2022-04-03 16:26:20 | INFO | hw5.seq2seq | validation loss:\t3.2641\n",
            "2022-04-03 16:26:20 | INFO | hw5.seq2seq | BLEU = 25.96 59.3/33.9/20.4/12.9 (BP = 0.963 ratio = 0.964 hyp_len = 107738 ref_len = 111811)\n",
            "2022-04-03 16:26:21 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/haoyu/Desktop/2022ML/hw5/checkpoints/rnn/checkpoint19.pt\n",
            "2022-04-03 16:26:21 | INFO | hw5.seq2seq | end of epoch 19\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6145771b694f4291b41f1f6177206cce",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": "train epoch 20:   0%|          | 0/791 [00:00<?, ?it/s]"
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-03 16:38:44 | INFO | hw5.seq2seq | training loss: 2.6234\n",
            "2022-04-03 16:38:44 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a0974afee6864077b0ee00603fcf36f5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": "validation:   0%|          | 0/22 [00:00<?, ?it/s]"
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-03 16:39:11 | INFO | hw5.seq2seq | example source: when the heat from the sun melted the wax on his wings , icarus fell from the sky .\n",
            "2022-04-03 16:39:11 | INFO | hw5.seq2seq | example hypothesis: 當太陽的熱能在他的翅膀上融化 , 伊卡洛斯從天空降下 。\n",
            "2022-04-03 16:39:11 | INFO | hw5.seq2seq | example reference: 當太陽的高溫融化了他翅膀上的蠟時 , 伊卡洛斯從天上掉下來 。\n",
            "2022-04-03 16:39:11 | INFO | hw5.seq2seq | validation loss:\t3.2754\n",
            "2022-04-03 16:39:11 | INFO | hw5.seq2seq | BLEU = 25.52 59.9/34.3/20.6/13.0 (BP = 0.937 ratio = 0.939 hyp_len = 104969 ref_len = 111811)\n",
            "2022-04-03 16:39:16 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/haoyu/Desktop/2022ML/hw5/checkpoints/rnn/checkpoint20.pt\n",
            "2022-04-03 16:39:16 | INFO | hw5.seq2seq | end of epoch 20\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b5e92f367e5349e383930f4af27ef39d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": "train epoch 21:   0%|          | 0/791 [00:00<?, ?it/s]"
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-03 16:51:40 | INFO | hw5.seq2seq | training loss: 2.5993\n",
            "2022-04-03 16:51:40 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e8f338e8a3904f8aa27891edbcf44d4f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": "validation:   0%|          | 0/22 [00:00<?, ?it/s]"
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-03 16:52:08 | INFO | hw5.seq2seq | example source: is migration really a choice ?\n",
            "2022-04-03 16:52:08 | INFO | hw5.seq2seq | example hypothesis: 遷徙真的是一個選擇嗎 ?\n",
            "2022-04-03 16:52:08 | INFO | hw5.seq2seq | example reference: 遷移真的是一個選擇嗎 ?\n",
            "2022-04-03 16:52:08 | INFO | hw5.seq2seq | validation loss:\t3.2905\n",
            "2022-04-03 16:52:08 | INFO | hw5.seq2seq | BLEU = 25.59 60.1/34.5/20.7/13.1 (BP = 0.935 ratio = 0.937 hyp_len = 104759 ref_len = 111811)\n",
            "2022-04-03 16:52:08 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/haoyu/Desktop/2022ML/hw5/checkpoints/rnn/checkpoint21.pt\n",
            "2022-04-03 16:52:08 | INFO | hw5.seq2seq | end of epoch 21\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e0af71a52884463b80c162468f0d466b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": "train epoch 22:   0%|          | 0/791 [00:00<?, ?it/s]"
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-03 17:04:31 | INFO | hw5.seq2seq | training loss: 2.5730\n",
            "2022-04-03 17:04:31 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7d92dd12b43849479cc2b678f0ad4410",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": "validation:   0%|          | 0/22 [00:00<?, ?it/s]"
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-03 17:04:59 | INFO | hw5.seq2seq | example source: he's saying to me , 'electricity . ' was he an electrician ? \" \" no . \"\n",
            "2022-04-03 17:04:59 | INFO | hw5.seq2seq | example hypothesis: 他對我說: 「 電力 。 他是電力專家嗎 ? 」 「 不是 。 」\n",
            "2022-04-03 17:04:59 | INFO | hw5.seq2seq | example reference: 他對我說『電流』他是一個電工嗎 ? 」 不是\n",
            "2022-04-03 17:04:59 | INFO | hw5.seq2seq | validation loss:\t3.2878\n",
            "2022-04-03 17:04:59 | INFO | hw5.seq2seq | BLEU = 25.98 59.1/33.8/20.3/12.8 (BP = 0.967 ratio = 0.968 hyp_len = 108226 ref_len = 111811)\n",
            "2022-04-03 17:05:00 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/haoyu/Desktop/2022ML/hw5/checkpoints/rnn/checkpoint22.pt\n",
            "2022-04-03 17:05:00 | INFO | hw5.seq2seq | end of epoch 22\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "96f4cd73d31345668b5dd47574572949",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": "train epoch 23:   0%|          | 0/791 [00:00<?, ?it/s]"
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-03 17:17:23 | INFO | hw5.seq2seq | training loss: 2.5512\n",
            "2022-04-03 17:17:23 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fcc870fca72045679b3fe37b8f699500",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": "validation:   0%|          | 0/22 [00:00<?, ?it/s]"
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-03 17:17:50 | INFO | hw5.seq2seq | example source: what is the most surprising thing about the experience of driving the car ?\n",
            "2022-04-03 17:17:50 | INFO | hw5.seq2seq | example hypothesis: 關於開車的經驗 , 最讓人驚訝的是什麼 ?\n",
            "2022-04-03 17:17:50 | INFO | hw5.seq2seq | example reference: 甚麼是開這台車時最讓人驚訝的體驗呢 ? 甚麼是開這台車時最讓人驚訝的體驗呢 ?\n",
            "2022-04-03 17:17:50 | INFO | hw5.seq2seq | validation loss:\t3.3088\n",
            "2022-04-03 17:17:50 | INFO | hw5.seq2seq | BLEU = 25.56 60.0/34.4/20.6/12.9 (BP = 0.939 ratio = 0.941 hyp_len = 105195 ref_len = 111811)\n",
            "2022-04-03 17:17:51 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/haoyu/Desktop/2022ML/hw5/checkpoints/rnn/checkpoint23.pt\n",
            "2022-04-03 17:17:51 | INFO | hw5.seq2seq | end of epoch 23\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ece205b10cac40baa44826e68cc63398",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": "train epoch 24:   0%|          | 0/791 [00:00<?, ?it/s]"
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-03 17:30:15 | INFO | hw5.seq2seq | training loss: 2.5277\n",
            "2022-04-03 17:30:15 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8d08df6f0ee944f9ab5a910245ea04d7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": "validation:   0%|          | 0/22 [00:00<?, ?it/s]"
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-03 17:30:43 | INFO | hw5.seq2seq | example source: either they have forgotten their way back to the hive , or they have simply disappeared .\n",
            "2022-04-03 17:30:43 | INFO | hw5.seq2seq | example hypothesis: 要不就是已經忘了回到蜂巢的路 , 要不就是就消失了 。\n",
            "2022-04-03 17:30:43 | INFO | hw5.seq2seq | example reference: 牠們不是早忘了回蜂巢的路就是單純的消失了\n",
            "2022-04-03 17:30:43 | INFO | hw5.seq2seq | validation loss:\t3.3078\n",
            "2022-04-03 17:30:43 | INFO | hw5.seq2seq | BLEU = 25.80 59.4/34.0/20.5/12.9 (BP = 0.955 ratio = 0.956 hyp_len = 106844 ref_len = 111811)\n",
            "2022-04-03 17:30:43 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/haoyu/Desktop/2022ML/hw5/checkpoints/rnn/checkpoint24.pt\n",
            "2022-04-03 17:30:43 | INFO | hw5.seq2seq | end of epoch 24\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ac35308180a645aca240ed30e9fddb57",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": "train epoch 25:   0%|          | 0/791 [00:00<?, ?it/s]"
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-03 17:43:06 | INFO | hw5.seq2seq | training loss: 2.5074\n",
            "2022-04-03 17:43:06 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c9d5b252ec6d4c5ea7948af6300883e6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": "validation:   0%|          | 0/22 [00:00<?, ?it/s]"
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-03 17:43:34 | INFO | hw5.seq2seq | example source: pay up , because we did it .\n",
            "2022-04-03 17:43:34 | INFO | hw5.seq2seq | example hypothesis: 付錢 , 因為我們做到了 。\n",
            "2022-04-03 17:43:34 | INFO | hw5.seq2seq | example reference: 交出來 , 因為我們做到了 。\n",
            "2022-04-03 17:43:34 | INFO | hw5.seq2seq | validation loss:\t3.3163\n",
            "2022-04-03 17:43:34 | INFO | hw5.seq2seq | BLEU = 25.79 59.0/33.8/20.3/12.8 (BP = 0.960 ratio = 0.961 hyp_len = 107473 ref_len = 111811)\n",
            "2022-04-03 17:43:35 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/haoyu/Desktop/2022ML/hw5/checkpoints/rnn/checkpoint25.pt\n",
            "2022-04-03 17:43:35 | INFO | hw5.seq2seq | end of epoch 25\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8ab21867320b4dc18f55e51c7dbf5b84",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": "train epoch 26:   0%|          | 0/791 [00:00<?, ?it/s]"
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-03 17:55:57 | INFO | hw5.seq2seq | training loss: 2.4881\n",
            "2022-04-03 17:55:57 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "eb0f9e94181440dd9ae3459cf3b32f40",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": "validation:   0%|          | 0/22 [00:00<?, ?it/s]"
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-03 17:56:25 | INFO | hw5.seq2seq | example source: but i realized that life could be very boring , so i've been thinking about life , and i notice that my camera my digital camera versus my car , a very strange thing .\n",
            "2022-04-03 17:56:25 | INFO | hw5.seq2seq | example hypothesis: 但我了解到 , 人生是很無聊的 , 所以我一直在思考人生 , 我注意到我的數位相機和我的車 , 是一件很奇怪的事 。\n",
            "2022-04-03 17:56:25 | INFO | hw5.seq2seq | example reference: 但我意識到這樣的生活可能會很無聊所以我一直思考著生活 , 突然注意到我的相機數位相機和我的車 , 我發現一件奇怪的事\n",
            "2022-04-03 17:56:25 | INFO | hw5.seq2seq | validation loss:\t3.3264\n",
            "2022-04-03 17:56:25 | INFO | hw5.seq2seq | BLEU = 25.65 59.3/33.9/20.3/12.8 (BP = 0.954 ratio = 0.955 hyp_len = 106792 ref_len = 111811)\n",
            "2022-04-03 17:56:26 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/haoyu/Desktop/2022ML/hw5/checkpoints/rnn/checkpoint26.pt\n",
            "2022-04-03 17:56:26 | INFO | hw5.seq2seq | end of epoch 26\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1667283c75f84e40afeb3013ca385bfd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": "train epoch 27:   0%|          | 0/791 [00:00<?, ?it/s]"
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-03 18:08:48 | INFO | hw5.seq2seq | training loss: 2.4698\n",
            "2022-04-03 18:08:48 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "35e48a0d9e414924985b1053babf9524",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": "validation:   0%|          | 0/22 [00:00<?, ?it/s]"
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-03 18:09:15 | INFO | hw5.seq2seq | example source: you may see 50 , 100 , 200 things on a shelf as you walk down it , but i have to work within that domain , to ensure that it gets you there first .\n",
            "2022-04-03 18:09:15 | INFO | hw5.seq2seq | example hypothesis: 你可能在架上看到50、100或200個東西但我必須在架上工作確保它首先到達\n",
            "2022-04-03 18:09:15 | INFO | hw5.seq2seq | example reference: 你可能看見架上擺著50、100、200件商品在逛商場的時候 , 但我必須精心設計 , 以確保我所做的商品能夠率先吸引你的目光 。\n",
            "2022-04-03 18:09:15 | INFO | hw5.seq2seq | validation loss:\t3.3525\n",
            "2022-04-03 18:09:15 | INFO | hw5.seq2seq | BLEU = 25.51 59.9/34.3/20.6/13.1 (BP = 0.936 ratio = 0.938 hyp_len = 104829 ref_len = 111811)\n",
            "2022-04-03 18:09:16 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/haoyu/Desktop/2022ML/hw5/checkpoints/rnn/checkpoint27.pt\n",
            "2022-04-03 18:09:16 | INFO | hw5.seq2seq | end of epoch 27\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "107f83e8d8204dee80df1adcf397a406",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": "train epoch 28:   0%|          | 0/791 [00:00<?, ?it/s]"
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-03 18:21:41 | INFO | hw5.seq2seq | training loss: 2.4527\n",
            "2022-04-03 18:21:41 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "03cd0bb031a74d8896c91528ce2f012f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": "validation:   0%|          | 0/22 [00:00<?, ?it/s]"
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-03 18:22:09 | INFO | hw5.seq2seq | example source: yet we still allow our mind to cycle through one greatest hit after another , like we were being held hostage by our own passiveaggressive spotify playlist .\n",
            "2022-04-03 18:22:09 | INFO | hw5.seq2seq | example hypothesis: 然而 , 我們仍允許我們的大腦在每次大受歡迎之後都循環 , 就像我們被動式的劇作家 , 主持人質 。\n",
            "2022-04-03 18:22:09 | INFO | hw5.seq2seq | example reference: 但我們仍然允許我們的大腦陷在這不斷重擊的循環中 , 彷彿我們被自己被動攻擊的spotify音樂播放清單給挾持當人質了 。\n",
            "2022-04-03 18:22:09 | INFO | hw5.seq2seq | validation loss:\t3.3454\n",
            "2022-04-03 18:22:09 | INFO | hw5.seq2seq | BLEU = 25.72 58.9/33.6/20.1/12.7 (BP = 0.965 ratio = 0.966 hyp_len = 107969 ref_len = 111811)\n",
            "2022-04-03 18:22:09 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/haoyu/Desktop/2022ML/hw5/checkpoints/rnn/checkpoint28.pt\n",
            "2022-04-03 18:22:09 | INFO | hw5.seq2seq | end of epoch 28\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "afbeeb69cd6a425ab1b2b02e95ba8ed2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": "train epoch 29:   0%|          | 0/791 [00:00<?, ?it/s]"
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-03 18:34:32 | INFO | hw5.seq2seq | training loss: 2.4347\n",
            "2022-04-03 18:34:32 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f622703b20324a729ea3177b2f40c0b5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": "validation:   0%|          | 0/22 [00:00<?, ?it/s]"
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-03 18:35:00 | INFO | hw5.seq2seq | example source: the magic of the mechanisms inside each genetic structure saying exactly where that nerve cell should go the complexity of these , the mathematical models of how these things are indeed done are beyond human comprehension .\n",
            "2022-04-03 18:35:00 | INFO | hw5.seq2seq | example hypothesis: 每一個基因結構的神奇之處就在於告訴神經細胞該去複雜的地方這些東西是如何被實現的數學模型超越人類理解的範圍\n",
            "2022-04-03 18:35:00 | INFO | hw5.seq2seq | example reference: 這些在每一個遺傳結構內準確的告訴神經細胞要去哪裡的機制 , 其不可思議之處在於--這些數學模型的複雜程度 , 這些事情是怎樣發生的遠遠超過人類所能理解 。 即使像我身為數學家 ,\n",
            "2022-04-03 18:35:00 | INFO | hw5.seq2seq | validation loss:\t3.3549\n",
            "2022-04-03 18:35:00 | INFO | hw5.seq2seq | BLEU = 25.59 59.4/33.9/20.4/12.9 (BP = 0.948 ratio = 0.949 hyp_len = 106131 ref_len = 111811)\n",
            "2022-04-03 18:35:00 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/haoyu/Desktop/2022ML/hw5/checkpoints/rnn/checkpoint29.pt\n",
            "2022-04-03 18:35:00 | INFO | hw5.seq2seq | end of epoch 29\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a5380a8e634c4f23aeaebc04cbdd3ca7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": "train epoch 30:   0%|          | 0/791 [00:00<?, ?it/s]"
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-03 18:47:24 | INFO | hw5.seq2seq | training loss: 2.4199\n",
            "2022-04-03 18:47:24 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "62e5d31c252f42bea30499a32eda97a5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": "validation:   0%|          | 0/22 [00:00<?, ?it/s]"
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-03 18:47:52 | INFO | hw5.seq2seq | example source: and i promise it's worth it .\n",
            "2022-04-03 18:47:52 | INFO | hw5.seq2seq | example hypothesis: 我保證它值得 。\n",
            "2022-04-03 18:47:52 | INFO | hw5.seq2seq | example reference: 我保證是值得的 。\n",
            "2022-04-03 18:47:52 | INFO | hw5.seq2seq | validation loss:\t3.3634\n",
            "2022-04-03 18:47:52 | INFO | hw5.seq2seq | BLEU = 25.70 58.6/33.3/19.9/12.5 (BP = 0.972 ratio = 0.973 hyp_len = 108754 ref_len = 111811)\n",
            "2022-04-03 18:47:53 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/haoyu/Desktop/2022ML/hw5/checkpoints/rnn/checkpoint30.pt\n",
            "2022-04-03 18:47:53 | INFO | hw5.seq2seq | end of epoch 30\n"
          ]
        }
      ],
      "source": [
        "epoch_itr = load_data_iterator(task, \"train\", config.start_epoch, config.max_tokens, config.num_workers)\n",
        "try_load_checkpoint(model, optimizer, name=config.resume)\n",
        "while epoch_itr.next_epoch_idx <= config.max_epoch:\n",
        "    # train for one epoch\n",
        "    train_one_epoch(epoch_itr, model, task, criterion, optimizer, config.accum_steps)\n",
        "    stats = validate_and_save(model, task, criterion, optimizer, epoch=epoch_itr.epoch)\n",
        "    logger.info(\"end of epoch {}\".format(epoch_itr.epoch))    \n",
        "    epoch_itr = load_data_iterator(task, \"train\", epoch_itr.next_epoch_idx, config.max_tokens, config.num_workers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KyjRwllxPjtf"
      },
      "source": [
        "# Submission"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "N70Gc6smPi1d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Namespace(checkpoint_upper_bound=None, inputs=['./checkpoints/rnn'], num_epoch_checkpoints=5, num_update_checkpoints=None, output='./checkpoints/rnn/avg_last_5_checkpoint.pt')\n",
            "averaging checkpoints:  ['./checkpoints/rnn/checkpoint30.pt', './checkpoints/rnn/checkpoint29.pt', './checkpoints/rnn/checkpoint28.pt', './checkpoints/rnn/checkpoint27.pt', './checkpoints/rnn/checkpoint26.pt']\n",
            "Finished writing averaged checkpoint to ./checkpoints/rnn/avg_last_5_checkpoint.pt\n"
          ]
        }
      ],
      "source": [
        "# averaging a few checkpoints can have a similar effect to ensemble\n",
        "checkdir=config.savedir\n",
        "!python3 ./fairseq/scripts/average_checkpoints.py \\\n",
        "--inputs {checkdir} \\\n",
        "--num-epoch-checkpoints 5 \\\n",
        "--output {checkdir}/avg_last_5_checkpoint.pt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BAGMiun8PnZy"
      },
      "source": [
        "## Confirm model weights used to generate submission"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "tvRdivVUPnsU"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-03 18:47:57 | INFO | hw5.seq2seq | loaded checkpoint checkpoints/rnn/avg_last_5_checkpoint.pt: step=unknown loss=3.3633663654327393 bleu=25.695822036281402\n",
            "2022-04-03 18:47:57 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5fdbf782a6454aeaae6463e637b3b571",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": "validation:   0%|          | 0/22 [00:00<?, ?it/s]"
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-03 18:48:24 | INFO | hw5.seq2seq | example source: arthur samuel knew strategy .\n",
            "2022-04-03 18:48:24 | INFO | hw5.seq2seq | example hypothesis: 亞瑟.山姆爾知道策略 。\n",
            "2022-04-03 18:48:24 | INFO | hw5.seq2seq | example reference: 他懂得策略 。\n",
            "2022-04-03 18:48:24 | INFO | hw5.seq2seq | validation loss:\t3.3083\n",
            "2022-04-03 18:48:24 | INFO | hw5.seq2seq | BLEU = 26.31 59.7/34.4/20.9/13.3 (BP = 0.958 ratio = 0.959 hyp_len = 107208 ref_len = 111811)\n"
          ]
        }
      ],
      "source": [
        "# checkpoint_last.pt : latest epoch\n",
        "# checkpoint_best.pt : highest validation bleu\n",
        "# avg_last_5_checkpoint.pt:　the average of last 5 epochs\n",
        "try_load_checkpoint(model, name=\"avg_last_5_checkpoint.pt\")\n",
        "validate(model, task, criterion, log_to_wandb=False)\n",
        "None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ioAIflXpPsxt"
      },
      "source": [
        "## Generate Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "oYMxA8FlPtIq"
      },
      "outputs": [],
      "source": [
        "def generate_prediction(model, task, split=\"test\", outfile=\"./prediction.txt\"):    \n",
        "    task.load_dataset(split=split, epoch=1)\n",
        "    itr = load_data_iterator(task, split, 1, config.max_tokens, config.num_workers).next_epoch_itr(shuffle=False)\n",
        "    \n",
        "    idxs = []\n",
        "    hyps = []\n",
        "\n",
        "    model.eval()\n",
        "    progress = tqdm.tqdm(itr, desc=f\"prediction\")\n",
        "    with torch.no_grad():\n",
        "        for i, sample in enumerate(progress):\n",
        "            # validation loss\n",
        "            sample = utils.move_to_cuda(sample, device=device)\n",
        "\n",
        "            # do inference\n",
        "            s, h, r = inference_step(sample, model)\n",
        "            \n",
        "            hyps.extend(h)\n",
        "            idxs.extend(list(sample['id']))\n",
        "            \n",
        "    # sort based on the order before preprocess\n",
        "    hyps = [x for _,x in sorted(zip(idxs,hyps))]\n",
        "    \n",
        "    with open(outfile, \"w\") as f:\n",
        "        for h in hyps:\n",
        "            f.write(h+\"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "Le4RFWXxjmm0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-03 18:48:25 | INFO | fairseq.data.data_utils | loaded 4,000 examples from: ./DATA/data-bin/ted2020/test.en-zh.en\n",
            "2022-04-03 18:48:25 | INFO | fairseq.data.data_utils | loaded 4,000 examples from: ./DATA/data-bin/ted2020/test.en-zh.zh\n",
            "2022-04-03 18:48:25 | INFO | fairseq.tasks.translation | ./DATA/data-bin/ted2020 test en-zh 4000 examples\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7fa64f7ba8194574a9edd9b6de2accba",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": "prediction:   0%|          | 0/17 [00:00<?, ?it/s]"
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "generate_prediction(model, task)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1z0cJE-wPzaU"
      },
      "source": [
        "# Back-translation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-7uPJ2CP0sm"
      },
      "source": [
        "## Train a backward translation model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ppGHjg2ZP3sV"
      },
      "source": [
        "1. Switch the source_lang and target_lang in **config** \n",
        "2. Change the savedir in **config** (eg. \"./checkpoints/transformer-back\")\n",
        "3. Train model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "waTGz29UP6WI"
      },
      "source": [
        "## Generate synthetic data with backward model "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sIeTsPexP8FL"
      },
      "source": [
        "### Download monolingual data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "i7N4QlsbP8fh"
      },
      "outputs": [],
      "source": [
        "mono_dataset_name = 'mono'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "396saD9-QBPY"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2022-04-03 18:48:49--  https://github.com/yuhsinchan/ML2022-HW5Dataset/releases/download/v1.0.2/ted_zh_corpus.deduped.gz\n",
            "正在查找主機 github.com (github.com)... 13.114.40.48\n",
            "正在連接 github.com (github.com)|13.114.40.48|:443... 連上了。\n",
            "已送出 HTTP 要求，正在等候回應... 302 Found\n",
            "位置：https://objects.githubusercontent.com/github-production-release-asset-2e65be/465173291/e0bb1e99-3b10-4346-b4b7-e1aa83404760?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20220403%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20220403T104850Z&X-Amz-Expires=300&X-Amz-Signature=58c212c21ad64e989a9c5dee2596328ece452480e1eafb500d35d39f225cb1b4&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=465173291&response-content-disposition=attachment%3B%20filename%3Dted_zh_corpus.deduped.gz&response-content-type=application%2Foctet-stream [跟隨至新的 URL]\n",
            "--2022-04-03 18:48:50--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/465173291/e0bb1e99-3b10-4346-b4b7-e1aa83404760?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20220403%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20220403T104850Z&X-Amz-Expires=300&X-Amz-Signature=58c212c21ad64e989a9c5dee2596328ece452480e1eafb500d35d39f225cb1b4&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=465173291&response-content-disposition=attachment%3B%20filename%3Dted_zh_corpus.deduped.gz&response-content-type=application%2Foctet-stream\n",
            "正在查找主機 objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.109.133, ...\n",
            "正在連接 objects.githubusercontent.com (objects.githubusercontent.com)|185.199.111.133|:443... 連上了。\n",
            "已送出 HTTP 要求，正在等候回應... 200 OK\n",
            "長度: 21709855 (21M) [application/octet-stream]\n",
            "儲存到：`/home/haoyu/Desktop/2022ML/hw5/DATA/rawdata/mono/ted_zh_corpus.deduped.gz'\n",
            "\n",
            "/home/haoyu/Desktop 100%[===================>]  20.70M   498KB/s    於 47s     \n",
            "\n",
            "2022-04-03 18:49:37 (451 KB/s) - 已儲存 `/home/haoyu/Desktop/2022ML/hw5/DATA/rawdata/mono/ted_zh_corpus.deduped.gz' [21709855/21709855]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "mono_prefix = Path(data_dir).absolute() / mono_dataset_name\n",
        "mono_prefix.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "urls = (\n",
        "    \"https://github.com/yuhsinchan/ML2022-HW5Dataset/releases/download/v1.0.2/ted_zh_corpus.deduped.gz\",\n",
        ")\n",
        "file_names = (\n",
        "    'ted_zh_corpus.deduped.gz',\n",
        ")\n",
        "\n",
        "for u, f in zip(urls, file_names):\n",
        "    path = mono_prefix/f\n",
        "    if not path.exists():\n",
        "        !wget {u} -O {path}\n",
        "    else:\n",
        "        print(f'{f} is exist, skip downloading')\n",
        "    if path.suffix == \".tgz\":\n",
        "        !tar -xvf {path} -C {prefix}\n",
        "    elif path.suffix == \".zip\":\n",
        "        !unzip -o {path} -d {prefix}\n",
        "    elif path.suffix == \".gz\":\n",
        "        !gzip -fkd {path}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JOVQRHzGQU4-"
      },
      "source": [
        "### TODO: clean corpus\n",
        "\n",
        "1. remove sentences that are too long or too short\n",
        "2. unify punctuation\n",
        "\n",
        "hint: you can use clean_s() defined above to do this"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "eIYmxfUOQSov"
      },
      "outputs": [],
      "source": [
        "def clean_mono_corpus(mono_prefix, l1, l2, max_len=1000, min_len=1):\n",
        "    if Path(f'{mono_prefix}/ted_zh_corpus.deduped.clean.{l1}').exists() and Path(f'{mono_prefix}/ted_zh_corpus.deduped.clean.{l2}').exists():\n",
        "        print(f'{mono_prefix}/ted_zh_corpus.deduped.clean.{l1} & {l2} exists. skipping clean.')\n",
        "        return\n",
        "    with open(f'{mono_prefix}/ted_zh_corpus.deduped', 'r') as l1_in_f:\n",
        "        with open(f'{mono_prefix}/ted_zh_corpus.deduped.clean.{l1}', 'w') as l1_out_f:\n",
        "            with open(f'{mono_prefix}/ted_zh_corpus.deduped.clean.{l2}', 'w') as l2_out_f:\n",
        "                for s1 in l1_in_f:\n",
        "                    s1 = s1.strip()\n",
        "                    s1 = clean_s(s1, l1)\n",
        "                    s1_len = len_s(s1, l1)\n",
        "                    if min_len > 0: # remove short sentence\n",
        "                        if s1_len < min_len:\n",
        "                            continue\n",
        "                    if max_len > 0: # remove long sentence\n",
        "                        if s1_len > max_len:\n",
        "                            continue\n",
        "                    print(s1, file=l1_out_f)\n",
        "                    print('.', file=l2_out_f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [],
      "source": [
        "clean_mono_corpus(mono_prefix, 'zh','en')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "非常謝謝你 , 克里斯 。 能有這個機會第二度踏上這個演講台\n",
            "真是一大榮幸 。 我非常感激 。\n",
            "這個研討會給我留下了極為深刻的印象 , 我想感謝大家對我之前演講的好評 。\n",
            "我是由衷的想這麼說 , 有部份原因是因為我真的有需要 !\n",
            "請你們設身處地為我想一想 !\n",
            "Thank you so much , Chris .\n",
            "And it's truly a great honor to have the opportunity to come to this stage twice ; I'm extremely grateful .\n",
            "I have been blown away by this conference , and I want to thank all of you for the many nice comments about what I had to say the other night .\n",
            "And I say that sincerely , partly because I need that .\n",
            "Put yourselves in my position .\n"
          ]
        }
      ],
      "source": [
        "!head {data_prefix + '.clean.' + 'zh'} -n 5\n",
        "!head {data_prefix + '.clean.' + 'en'} -n 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jegH0bvMQVmR"
      },
      "source": [
        "### TODO: Subword Units\n",
        "\n",
        "Use the spm model of the backward model to tokenize the data into subword units\n",
        "\n",
        "hint: spm model is located at DATA/raw-data/\\[dataset\\]/spm\\[vocab_num\\].model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "vqgR4uUMQZGY"
      },
      "outputs": [],
      "source": [
        "for lang in ['zh','en']:\n",
        "    out_path = mono_prefix/f'mono.tok.{lang}'\n",
        "    if out_path.exists():\n",
        "        print(f\"{out_path} exists. skipping spm_encode.\")\n",
        "    else:\n",
        "        with open(mono_prefix/f'mono.tok.{lang}', 'w') as out_f:\n",
        "            with open(mono_prefix/f'ted_zh_corpus.deduped.clean.{lang}', 'r') as in_f:\n",
        "                for line in in_f:\n",
        "                    line = line.strip()\n",
        "                    tok = spm_model.encode(line, out_type=str)\n",
        "                    print(' '.join(tok), file=out_f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a65glBVXQZiE"
      },
      "source": [
        "### Binarize\n",
        "\n",
        "use fairseq to binarize data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "b803qA5aQaEu"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-03 18:50:00 | INFO | fairseq_cli.preprocess | Namespace(align_suffix=None, alignfile=None, all_gather_list_size=16384, azureml_logging=False, bf16=False, bpe=None, cpu=False, criterion='cross_entropy', dataset_impl='mmap', destdir='DATA/data-bin/mono', empty_cache_freq=0, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, joined_dictionary=False, log_format=None, log_interval=100, lr_scheduler='fixed', memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_progress_bar=False, nwordssrc=-1, nwordstgt=-1, only_source=False, optimizer=None, padding_factor=8, profile=False, quantization_config_path=None, reset_logging=False, scoring='bleu', seed=1, source_lang='zh', srcdict='./DATA/data-bin/ted2020/dict.en.txt', suppress_crashes=False, target_lang='en', task='translation', tensorboard_logdir=None, testpref=None, tgtdict='./DATA/data-bin/ted2020/dict.en.txt', threshold_loss_scale=None, thresholdsrc=0, thresholdtgt=0, tokenizer=None, tpu=False, trainpref='/home/haoyu/Desktop/2022ML/hw5/DATA/rawdata/mono/mono.tok', user_dir=None, validpref=None, wandb_project=None, workers=2)\n",
            "2022-04-03 18:50:00 | INFO | fairseq_cli.preprocess | [zh] Dictionary: 8000 types\n",
            "2022-04-03 18:50:32 | INFO | fairseq_cli.preprocess | [zh] /home/haoyu/Desktop/2022ML/hw5/DATA/rawdata/mono/mono.tok.zh: 781713 sents, 14003299 tokens, 0.0023% replaced by <unk>\n",
            "2022-04-03 18:50:32 | INFO | fairseq_cli.preprocess | [en] Dictionary: 8000 types\n",
            "2022-04-03 18:50:40 | INFO | fairseq_cli.preprocess | [en] /home/haoyu/Desktop/2022ML/hw5/DATA/rawdata/mono/mono.tok.en: 781713 sents, 1563426 tokens, 0.0% replaced by <unk>\n",
            "2022-04-03 18:50:40 | INFO | fairseq_cli.preprocess | Wrote preprocessed data to DATA/data-bin/mono\n"
          ]
        }
      ],
      "source": [
        "binpath = Path('./DATA/data-bin', mono_dataset_name)\n",
        "src_dict_file = './DATA/data-bin/ted2020/dict.en.txt'\n",
        "tgt_dict_file = src_dict_file\n",
        "monopref = str(mono_prefix/\"mono.tok\") # whatever filepath you get after applying subword tokenization\n",
        "\n",
        "if binpath.exists():\n",
        "    print(binpath, \"exists, will not overwrite!\")\n",
        "else:\n",
        "    !python3 -m fairseq_cli.preprocess\\\n",
        "        --source-lang 'zh'\\\n",
        "        --target-lang 'en'\\\n",
        "        --trainpref {monopref}\\\n",
        "        --destdir {binpath}\\\n",
        "        --srcdict {src_dict_file}\\\n",
        "        --tgtdict {tgt_dict_file}\\\n",
        "        --workers 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "smA0JraEQdxz"
      },
      "source": [
        "### TODO: Generate synthetic data with backward model\n",
        "\n",
        "Add binarized monolingual data to the original data directory, and name it with \"split_name\"\n",
        "\n",
        "ex. ./DATA/data-bin/ted2020/\\[split_name\\].zh-en.\\[\"en\", \"zh\"\\].\\[\"bin\", \"idx\"\\]\n",
        "\n",
        "then you can use 'generate_prediction(model, task, split=\"split_name\")' to generate translation prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "jvaOVHeoQfkB"
      },
      "outputs": [],
      "source": [
        "# Add binarized monolingual data to the original data directory, and name it with \"split_name\"\n",
        "# ex. ./DATA/data-bin/ted2020/\\[split_name\\].zh-en.\\[\"en\", \"zh\"\\].\\[\"bin\", \"idx\"\\]\n",
        "!cp ./DATA/data-bin/mono/train.zh-en.zh.bin ./DATA/data-bin/ted2020/mono.zh-en.zh.bin\n",
        "!cp ./DATA/data-bin/mono/train.zh-en.zh.idx ./DATA/data-bin/ted2020/mono.zh-en.zh.idx\n",
        "!cp ./DATA/data-bin/mono/train.zh-en.en.bin ./DATA/data-bin/ted2020/mono.zh-en.en.bin\n",
        "!cp ./DATA/data-bin/mono/train.zh-en.en.idx ./DATA/data-bin/ted2020/mono.zh-en.en.idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "fFEkxPu-Qhlc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-03 18:50:43 | INFO | fairseq.data.data_utils | loaded 781,713 examples from: ./DATA/data-bin/ted2020/mono.zh-en.en\n",
            "2022-04-03 18:50:43 | INFO | fairseq.data.data_utils | loaded 781,713 examples from: ./DATA/data-bin/ted2020/mono.zh-en.zh\n",
            "2022-04-03 18:50:43 | INFO | fairseq.tasks.translation | ./DATA/data-bin/ted2020 mono en-zh 781713 examples\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ab151a2c9cb24d5983f9855386700794",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": "prediction:   0%|          | 0/1715 [00:00<?, ?it/s]"
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# hint: do prediction on split='mono' to create prediction_file\n",
        "generate_prediction( model, task ,split=\"mono\" ,outfile=\"./DATA/rawdata/mono/mono_prediction.txt\" )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "...\n",
            "...\n",
            "...\n",
            "...\n",
            "...\n"
          ]
        }
      ],
      "source": [
        "!head {'./DATA/rawdata/mono/mono_prediction.txt'} -n 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jn4XeawpQjLk"
      },
      "source": [
        "### TODO: Create new dataset\n",
        "\n",
        "1. Combine the prediction data with monolingual data\n",
        "2. Use the original spm model to tokenize data into Subword Units\n",
        "3. Binarize data with fairseq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "3R35JTaTQjkm"
      },
      "outputs": [],
      "source": [
        "# Combine prediction_file (.en) and mono.zh (.zh) into a new dataset.\n",
        "# \n",
        "# hint: tokenize prediction_file with the spm model\n",
        "# spm_model.encode(line, out_type=str)\n",
        "# output: ./DATA/rawdata/mono/mono.tok.en & mono.tok.zh\n",
        "with open(mono_prefix/f'mono.tok.en', 'w') as out_f:\n",
        "    with open('./DATA/rawdata/mono/mono_prediction.txt', 'r') as in_f:\n",
        "        for line in in_f:\n",
        "            line = line.strip()\n",
        "            tok = spm_model.encode(line, out_type=str)\n",
        "            print(' '.join(tok), file=out_f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DATA/data-bin/synthetic exists, will not overwrite!\n"
          ]
        }
      ],
      "source": [
        "# hint: use fairseq to binarize these two files again\n",
        "binpath = Path('./DATA/data-bin/synthetic')\n",
        "src_dict_file = './DATA/data-bin/ted2020/dict.en.txt'\n",
        "tgt_dict_file = src_dict_file\n",
        "monopref = Path('./DATA/rawdata/mono/mono.tok') # or whatever path after applying subword tokenization, w/o the suffix (.zh/.en)\n",
        "if binpath.exists():\n",
        "    print(binpath, \"exists, will not overwrite!\")\n",
        "else:\n",
        "    !python3 -m fairseq_cli.preprocess\\\n",
        "         --source-lang 'zh'\\\n",
        "         --target-lang 'en'\\\n",
        "         --trainpref {monopref}\\\n",
        "         --destdir {binpath}\\\n",
        "         --srcdict {src_dict_file}\\\n",
        "         --tgtdict {tgt_dict_file}\\\n",
        "         --workers 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "MSkse1tyQnsR"
      },
      "outputs": [],
      "source": [
        "# create a new dataset from all the files prepared above\n",
        "!cp -r ./DATA/data-bin/ted2020/ ./DATA/data-bin/ted2020_with_mono/\n",
        "\n",
        "!cp ./DATA/data-bin/synthetic/train.zh-en.zh.bin ./DATA/data-bin/ted2020_with_mono/train1.en-zh.zh.bin\n",
        "!cp ./DATA/data-bin/synthetic/train.zh-en.zh.idx ./DATA/data-bin/ted2020_with_mono/train1.en-zh.zh.idx\n",
        "!cp ./DATA/data-bin/synthetic/train.zh-en.en.bin ./DATA/data-bin/ted2020_with_mono/train1.en-zh.en.bin\n",
        "!cp ./DATA/data-bin/synthetic/train.zh-en.en.idx ./DATA/data-bin/ted2020_with_mono/train1.en-zh.en.idx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVdxVGO3QrSs"
      },
      "source": [
        "Created new dataset \"ted2020_with_mono\"\n",
        "\n",
        "1. Change the datadir in **config** (\"./DATA/data-bin/ted2020_with_mono\")\n",
        "2. Switch back the source_lang and target_lang in **config** (\"en\", \"zh\")\n",
        "2. Change the savedir in **config** (eg. \"./checkpoints/transformer-bt\")\n",
        "3. Train model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_CZU2beUQtl3"
      },
      "source": [
        "1. <a name=ott2019fairseq></a>Ott, M., Edunov, S., Baevski, A., Fan, A., Gross, S., Ng, N., ... & Auli, M. (2019, June). fairseq: A Fast, Extensible Toolkit for Sequence Modeling. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations) (pp. 48-53).\n",
        "2. <a name=vaswani2017></a>Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... & Polosukhin, I. (2017, December). Attention is all you need. In Proceedings of the 31st International Conference on Neural Information Processing Systems (pp. 6000-6010).\n",
        "3. <a name=reimers-2020-multilingual-sentence-bert></a>Reimers, N., & Gurevych, I. (2020, November). Making Monolingual Sentence Embeddings Multilingual Using Knowledge Distillation. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) (pp. 4512-4525).\n",
        "4. <a name=tiedemann2012parallel></a>Tiedemann, J. (2012, May). Parallel Data, Tools and Interfaces in OPUS. In Lrec (Vol. 2012, pp. 2214-2218).\n",
        "5. <a name=kudo-richardson-2018-sentencepiece></a>Kudo, T., & Richardson, J. (2018, November). SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations (pp. 66-71).\n",
        "6. <a name=sennrich-etal-2016-improving></a>Sennrich, R., Haddow, B., & Birch, A. (2016, August). Improving Neural Machine Translation Models with Monolingual Data. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) (pp. 86-96).\n",
        "7. <a name=edunov-etal-2018-understanding></a>Edunov, S., Ott, M., Auli, M., & Grangier, D. (2018). Understanding Back-Translation at Scale. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing (pp. 489-500).\n",
        "8. https://github.com/ajinkyakulkarni14/TED-Multilingual-Parallel-Corpus\n",
        "9. https://ithelp.ithome.com.tw/articles/10233122\n",
        "10. https://nlp.seas.harvard.edu/2018/04/03/attention.html\n",
        "11. https://colab.research.google.com/github/ga642381/ML2021-Spring/blob/main/HW05/HW05.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "Rrfm6iLJQ0tS"
      },
      "outputs": [],
      "source": [
        "config = Namespace(\n",
        "    datadir = \"./DATA/data-bin/ted2020_with_mono\",\n",
        "    savedir = \"./checkpoints/transformer-big\",\n",
        "    source_lang = \"en\",\n",
        "    target_lang = \"zh\",\n",
        "    \n",
        "    # cpu threads when fetching & processing data.\n",
        "    num_workers=2,  \n",
        "    # batch size in terms of tokens. gradient accumulation increases the effective batchsize.\n",
        "    max_tokens=4096,\n",
        "    accum_steps=2,\n",
        "    \n",
        "    # the lr s calculated from Noam lr scheduler. you can tune the maximum lr by this factor.\n",
        "    lr_factor=2.,\n",
        "    lr_warmup=4000,\n",
        "    \n",
        "    # clipping gradient norm helps alleviate gradient exploding\n",
        "    clip_norm=1.0,\n",
        "    \n",
        "    # maximum epochs for training\n",
        "    max_epoch=30,\n",
        "    start_epoch=1,\n",
        "    \n",
        "    # beam size for beam search\n",
        "    beam=5, \n",
        "    # generate sequences of maximum length ax + b, where x is the source length\n",
        "    max_len_a=1.2, \n",
        "    max_len_b=10, \n",
        "    # when decoding, post process sentence by removing sentencepiece symbols and jieba tokenization.\n",
        "    post_process = \"sentencepiece\",\n",
        "    \n",
        "    # checkpoints\n",
        "    keep_last_epochs=5,\n",
        "    resume=None, # if resume from checkpoint name (under config.savedir)\n",
        "    \n",
        "    # logging\n",
        "    use_wandb=False,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-03 19:13:18 | INFO | fairseq.tasks.translation | [en] dictionary: 8000 types\n",
            "2022-04-03 19:13:18 | INFO | fairseq.tasks.translation | [zh] dictionary: 8000 types\n"
          ]
        }
      ],
      "source": [
        "## setup task\n",
        "task_cfg = TranslationConfig(\n",
        "    data=config.datadir,\n",
        "    source_lang=config.source_lang,\n",
        "    target_lang=config.target_lang,\n",
        "    train_subset=\"train\",\n",
        "    required_seq_len_multiple=8,\n",
        "    dataset_impl=\"mmap\",\n",
        "    upsample_primary=1,\n",
        ")\n",
        "task = TranslationTask.setup_task(task_cfg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-03 19:13:19 | INFO | hw5.seq2seq | loading data for epoch 1\n",
            "2022-04-03 19:13:19 | INFO | fairseq.data.data_utils | loaded 390,041 examples from: ./DATA/data-bin/ted2020_with_mono/train.en-zh.en\n",
            "2022-04-03 19:13:19 | INFO | fairseq.data.data_utils | loaded 390,041 examples from: ./DATA/data-bin/ted2020_with_mono/train.en-zh.zh\n",
            "2022-04-03 19:13:19 | INFO | fairseq.tasks.translation | ./DATA/data-bin/ted2020_with_mono train en-zh 390041 examples\n",
            "2022-04-03 19:13:19 | INFO | fairseq.data.data_utils | loaded 781,713 examples from: ./DATA/data-bin/ted2020_with_mono/train1.en-zh.en\n",
            "2022-04-03 19:13:19 | INFO | fairseq.data.data_utils | loaded 781,713 examples from: ./DATA/data-bin/ted2020_with_mono/train1.en-zh.zh\n",
            "2022-04-03 19:13:19 | INFO | fairseq.tasks.translation | ./DATA/data-bin/ted2020_with_mono train1 en-zh 781713 examples\n",
            "2022-04-03 19:13:19 | INFO | fairseq.data.data_utils | loaded 3,939 examples from: ./DATA/data-bin/ted2020_with_mono/valid.en-zh.en\n",
            "2022-04-03 19:13:19 | INFO | fairseq.data.data_utils | loaded 3,939 examples from: ./DATA/data-bin/ted2020_with_mono/valid.en-zh.zh\n",
            "2022-04-03 19:13:19 | INFO | fairseq.tasks.translation | ./DATA/data-bin/ted2020_with_mono valid en-zh 3939 examples\n"
          ]
        }
      ],
      "source": [
        "logger.info(\"loading data for epoch 1\")\n",
        "task.load_dataset(split=\"train\", epoch=1, combine=True) # combine if you have back-translation data.\n",
        "task.load_dataset(split=\"valid\", epoch=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'id': 1,\n",
            " 'source': tensor([  18,   14,    6, 2234,   60,   19,   80,    5,  256,   16,  405, 1407,\n",
            "        1706,    7,    2]),\n",
            " 'target': tensor([ 140,  690,   28,  270,   45,  151, 1142,  660,  606,  369, 3114, 2434,\n",
            "        1434,  192,    2])}\n",
            "\"Source: that's exactly what i do optical mind control .\"\n",
            "'Target: 這實在就是我所做的--光學操控思想'\n"
          ]
        }
      ],
      "source": [
        "\n",
        "sample = task.dataset(\"valid\")[1]\n",
        "pprint.pprint(sample)\n",
        "pprint.pprint(\n",
        "    \"Source: \" + \\\n",
        "    task.source_dictionary.string(\n",
        "        sample['source'],\n",
        "        config.post_process,\n",
        "    )\n",
        ")\n",
        "pprint.pprint(\n",
        "    \"Target: \" + \\\n",
        "    task.target_dictionary.string(\n",
        "        sample['target'],\n",
        "        config.post_process,\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-03 19:13:21 | WARNING | fairseq.tasks.fairseq_task | 2,532 samples have invalid sizes and will be skipped, max_positions=(20, 20), first few sample ids=[29, 135, 2444, 3058, 682, 731, 235, 559, 3383, 1558]\n"
          ]
        },
        {
          "data": {
            "text/plain": "{'id': tensor([853]),\n 'nsentences': 1,\n 'ntokens': 14,\n 'net_input': {'src_tokens': tensor([[   1,    1,    1,    1,  466,   14,    6,  178,  150,   22,   63,   80,\n             20,   61,  896,    6,    6,  144,   29, 2249,   42,   25,    7,    2]]),\n  'src_lengths': tensor([20]),\n  'prev_output_tokens': tensor([[   2,   53,   78, 1255,    4,    5, 1245,  555,  891,  369,  118,  162,\n           1518,  380,    1,    1]])},\n 'target': tensor([[  53,   78, 1255,    4,    5, 1245,  555,  891,  369,  118,  162, 1518,\n           380,    2,    1,    1]])}"
          },
          "execution_count": 66,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "demo_epoch_obj = load_data_iterator(task, \"valid\", epoch=1, max_tokens=20, num_workers=1, cached=False)\n",
        "demo_iter = demo_epoch_obj.next_epoch_itr(shuffle=True)\n",
        "sample = next(demo_iter)\n",
        "sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {},
      "outputs": [],
      "source": [
        "arch_args = Namespace(\n",
        "    encoder_embed_dim=512,\n",
        "    encoder_ffn_embed_dim=2048,\n",
        "    encoder_layers=6,\n",
        "    decoder_embed_dim=512,\n",
        "    decoder_ffn_embed_dim=2048,\n",
        "    decoder_layers=6,\n",
        "    share_decoder_input_output_embed=True,\n",
        "    dropout=0.1,\n",
        ")\n",
        "\n",
        "# HINT: these patches on parameters for Transformer\n",
        "def add_transformer_args(args):\n",
        "    args.encoder_attention_heads=4\n",
        "    args.encoder_normalize_before=True\n",
        "    \n",
        "    args.decoder_attention_heads=4\n",
        "    args.decoder_normalize_before=True\n",
        "    \n",
        "    args.activation_fn=\"relu\"\n",
        "    args.max_source_positions=1024\n",
        "    args.max_target_positions=1024\n",
        "    \n",
        "    # patches on default parameters for Transformer (those not set above)\n",
        "    from fairseq.models.transformer import base_architecture\n",
        "    base_architecture(arch_args)\n",
        "\n",
        "add_transformer_args(arch_args)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-03 19:13:22 | INFO | hw5.seq2seq | Seq2Seq(\n",
            "  (encoder): TransformerEncoder(\n",
            "    (dropout_module): FairseqDropout()\n",
            "    (embed_tokens): Embedding(8000, 512, padding_idx=1)\n",
            "    (embed_positions): SinusoidalPositionalEmbedding()\n",
            "    (layers): ModuleList(\n",
            "      (0): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (1): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (2): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (3): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (4): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (5): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "    (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "  )\n",
            "  (decoder): TransformerDecoder(\n",
            "    (dropout_module): FairseqDropout()\n",
            "    (embed_tokens): Embedding(8000, 512, padding_idx=1)\n",
            "    (embed_positions): SinusoidalPositionalEmbedding()\n",
            "    (layers): ModuleList(\n",
            "      (0): TransformerDecoderLayer(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (1): TransformerDecoderLayer(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (2): TransformerDecoderLayer(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (3): TransformerDecoderLayer(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (4): TransformerDecoderLayer(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (5): TransformerDecoderLayer(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "    (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "    (output_projection): Linear(in_features=512, out_features=8000, bias=False)\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "model = build_model(arch_args, task)\n",
        "logger.info(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {},
      "outputs": [],
      "source": [
        "# generally, 0.1 is good enough\n",
        "criterion = LabelSmoothedCrossEntropyCriterion(\n",
        "    smoothing=0.1,\n",
        "    ignore_index=task.target_dictionary.pad(),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {},
      "outputs": [],
      "source": [
        "optimizer = NoamOpt(\n",
        "    model_size=arch_args.encoder_embed_dim, \n",
        "    factor=config.lr_factor, \n",
        "    warmup=config.lr_warmup, \n",
        "    optimizer=torch.optim.AdamW(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9, weight_decay=0.0001))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {},
      "outputs": [],
      "source": [
        "sequence_generator = task.build_generator([model], config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = model.to(device=device)\n",
        "criterion = criterion.to(device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-03 19:13:24 | INFO | hw5.seq2seq | task: TranslationTask\n",
            "2022-04-03 19:13:24 | INFO | hw5.seq2seq | encoder: TransformerEncoder\n",
            "2022-04-03 19:13:24 | INFO | hw5.seq2seq | decoder: TransformerDecoder\n",
            "2022-04-03 19:13:24 | INFO | hw5.seq2seq | criterion: LabelSmoothedCrossEntropyCriterion\n",
            "2022-04-03 19:13:24 | INFO | hw5.seq2seq | optimizer: NoamOpt\n",
            "2022-04-03 19:13:24 | INFO | hw5.seq2seq | num. model params: 52,332,544 (num. trained: 52,332,544)\n",
            "2022-04-03 19:13:24 | INFO | hw5.seq2seq | max tokens per batch = 4096, accumulate steps = 2\n"
          ]
        }
      ],
      "source": [
        "logger.info(\"task: {}\".format(task.__class__.__name__))\n",
        "logger.info(\"encoder: {}\".format(model.encoder.__class__.__name__))\n",
        "logger.info(\"decoder: {}\".format(model.decoder.__class__.__name__))\n",
        "logger.info(\"criterion: {}\".format(criterion.__class__.__name__))\n",
        "logger.info(\"optimizer: {}\".format(optimizer.__class__.__name__))\n",
        "logger.info(\n",
        "    \"num. model params: {:,} (num. trained: {:,})\".format(\n",
        "        sum(p.numel() for p in model.parameters()),\n",
        "        sum(p.numel() for p in model.parameters() if p.requires_grad),\n",
        "    )\n",
        ")\n",
        "logger.info(f\"max tokens per batch = {config.max_tokens}, accumulate steps = {config.accum_steps}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-03 19:13:26 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[326674]\n",
            "2022-04-03 19:13:26 | INFO | hw5.seq2seq | no checkpoints found at checkpoints/transformer-big/checkpoint_last.pt!\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6c8337dd7040484fb852053e7f3e0641",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": "train epoch 1:   0%|          | 0/3266 [00:00<?, ?it/s]"
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-03 19:39:10 | INFO | hw5.seq2seq | training loss: 5.4605\n",
            "2022-04-03 19:39:10 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9fb4acf034af45618bf2c827fb5ae45f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": "validation:   0%|          | 0/40 [00:00<?, ?it/s]"
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-03 19:39:35 | INFO | hw5.seq2seq | example source: congratulations .\n",
            "2022-04-03 19:39:35 | INFO | hw5.seq2seq | example hypothesis: 相反 。\n",
            "2022-04-03 19:39:35 | INFO | hw5.seq2seq | example reference: 恭喜各位 。\n",
            "2022-04-03 19:39:35 | INFO | hw5.seq2seq | validation loss:\t4.4131\n",
            "2022-04-03 19:39:35 | INFO | hw5.seq2seq | BLEU = 6.82 42.0/16.6/7.5/3.6 (BP = 0.583 ratio = 0.650 hyp_len = 72627 ref_len = 111811)\n",
            "2022-04-03 19:39:35 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/haoyu/Desktop/2022ML/hw5/checkpoints/transformer-big/checkpoint1.pt\n",
            "2022-04-03 19:39:35 | INFO | hw5.seq2seq | end of epoch 1\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4f04e214d605411c88933763eeabddd5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": "train epoch 2:   0%|          | 0/3266 [00:00<?, ?it/s]"
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-03 20:05:15 | INFO | hw5.seq2seq | training loss: 4.5729\n",
            "2022-04-03 20:05:15 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4a9a5ca1bc0445cfa37bd04b7a7ecaee",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": "validation:   0%|          | 0/40 [00:00<?, ?it/s]"
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-03 20:05:49 | INFO | hw5.seq2seq | example source: but i'll tell you some of that today .\n",
            "2022-04-03 20:05:49 | INFO | hw5.seq2seq | example hypothesis: 但今天我要告訴各位 , 今天我要告訴你們一些 。\n",
            "2022-04-03 20:05:49 | INFO | hw5.seq2seq | example reference: 但我今天會告訴你們其中的一部分 。\n",
            "2022-04-03 20:05:49 | INFO | hw5.seq2seq | validation loss:\t3.7824\n",
            "2022-04-03 20:05:49 | INFO | hw5.seq2seq | BLEU = 14.68 40.9/19.4/10.2/5.7 (BP = 1.000 ratio = 1.029 hyp_len = 115045 ref_len = 111811)\n",
            "2022-04-03 20:05:50 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/haoyu/Desktop/2022ML/hw5/checkpoints/transformer-big/checkpoint2.pt\n",
            "2022-04-03 20:05:50 | INFO | hw5.seq2seq | end of epoch 2\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e93e2588cef04ea58f072af089fe5eaf",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": "train epoch 3:   0%|          | 0/3266 [00:00<?, ?it/s]"
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-03 20:31:26 | INFO | hw5.seq2seq | training loss: 4.2564\n",
            "2022-04-03 20:31:26 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c43b6df208954afda1146ad6aa935a1b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": "validation:   0%|          | 0/40 [00:00<?, ?it/s]"
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-03 20:31:57 | INFO | hw5.seq2seq | example source: howard is a man of a certain degree of intellectual standards .\n",
            "2022-04-03 20:31:57 | INFO | hw5.seq2seq | example hypothesis: 法蘭克是某種智能標準的標準 。\n",
            "2022-04-03 20:31:57 | INFO | hw5.seq2seq | example reference: 默斯克韋茲是個秉持某程度的智慧標準的人 。\n",
            "2022-04-03 20:31:57 | INFO | hw5.seq2seq | validation loss:\t3.5137\n",
            "2022-04-03 20:31:57 | INFO | hw5.seq2seq | BLEU = 17.96 49.5/25.5/14.3/8.5 (BP = 0.909 ratio = 0.913 hyp_len = 102068 ref_len = 111811)\n",
            "2022-04-03 20:31:58 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/haoyu/Desktop/2022ML/hw5/checkpoints/transformer-big/checkpoint3.pt\n",
            "2022-04-03 20:31:58 | INFO | hw5.seq2seq | end of epoch 3\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f609f6ae5386438cb713382283c868e2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": "train epoch 4:   0%|          | 0/3266 [00:00<?, ?it/s]"
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f576649d790>\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/haoyu/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 1358, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/home/haoyu/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 1341, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 160, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "AssertionError: can only test a child process\n",
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f576649d790>\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/haoyu/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 1358, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/home/haoyu/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 1341, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 160, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "AssertionError: can only test a child process\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-03 20:57:33 | INFO | hw5.seq2seq | training loss: 4.0999\n",
            "2022-04-03 20:57:33 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "58c1cf88c41e42b089d2877a35a53295",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": "validation:   0%|          | 0/40 [00:00<?, ?it/s]"
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-03 20:58:04 | INFO | hw5.seq2seq | example source: but humans are curious , and they like to add things to their bodies so they can go to the alps one day and then become a fish in the sea the next .\n",
            "2022-04-03 20:58:04 | INFO | hw5.seq2seq | example hypothesis: 但人類是好奇的 , 他們喜歡把東西加入他們的身體中 , 讓它們有一天可以到達海邊 , 然後成為魚類 。\n",
            "2022-04-03 20:58:04 | INFO | hw5.seq2seq | example reference: 但人們好奇心又強 , 喜歡在身體上加掛東西這樣一來可以爬上阿爾卑斯山又可以下海游 。\n",
            "2022-04-03 20:58:04 | INFO | hw5.seq2seq | validation loss:\t3.3720\n",
            "2022-04-03 20:58:04 | INFO | hw5.seq2seq | BLEU = 20.63 51.4/27.3/15.5/9.4 (BP = 0.970 ratio = 0.970 hyp_len = 108478 ref_len = 111811)\n",
            "2022-04-03 20:58:05 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/haoyu/Desktop/2022ML/hw5/checkpoints/transformer-big/checkpoint4.pt\n",
            "2022-04-03 20:58:05 | INFO | hw5.seq2seq | end of epoch 4\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b0bd57b640ba4e3bbc8b2c7fce53e789",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": "train epoch 5:   0%|          | 0/3266 [00:00<?, ?it/s]"
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f576649d790>\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/haoyu/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 1358, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/home/haoyu/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 1341, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 160, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "AssertionError: can only test a child process\n",
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f576649d790>\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/haoyu/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 1358, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/home/haoyu/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 1341, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 160, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "AssertionError: can only test a child process\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-03 21:23:43 | INFO | hw5.seq2seq | training loss: 4.0014\n",
            "2022-04-03 21:23:43 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "afea8ece5a0a4d12af0c8b489b2728c5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": "validation:   0%|          | 0/40 [00:00<?, ?it/s]"
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-03 21:24:12 | INFO | hw5.seq2seq | example source: so last fall when we published the results of that work in science , we all became overconfident and were sure we were only a few weeks away from being able to now boot up a chromosome out of yeast .\n",
            "2022-04-03 21:24:12 | INFO | hw5.seq2seq | example hypothesis: 所以在我們發表了科學研究的結果後 , 我們都變得過度自信 , 只不過幾個禮拜後才能夠從酵母中取出染色體 。\n",
            "2022-04-03 21:24:12 | INFO | hw5.seq2seq | example reference: 於是去年秋天我們在《科學雜誌》發表該研究成果時我們都變得過於自信確信我們只須再幾個星期的時間就能夠啟動從酵母中取出的染色體\n",
            "2022-04-03 21:24:12 | INFO | hw5.seq2seq | validation loss:\t3.3041\n",
            "2022-04-03 21:24:12 | INFO | hw5.seq2seq | BLEU = 20.96 55.4/30.0/17.4/10.7 (BP = 0.890 ratio = 0.896 hyp_len = 100184 ref_len = 111811)\n",
            "2022-04-03 21:24:13 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/haoyu/Desktop/2022ML/hw5/checkpoints/transformer-big/checkpoint5.pt\n",
            "2022-04-03 21:24:13 | INFO | hw5.seq2seq | end of epoch 5\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5b456b79de57437eb89b63253f30c3c5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": "train epoch 6:   0%|          | 0/3266 [00:00<?, ?it/s]"
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-03 21:49:51 | INFO | hw5.seq2seq | training loss: 3.9310\n",
            "2022-04-03 21:49:51 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e75b5ea39160463d995e1d5b2751aeb0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": "validation:   0%|          | 0/40 [00:00<?, ?it/s]"
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-03 21:50:22 | INFO | hw5.seq2seq | example source: they don't always even know why they're doing it .\n",
            "2022-04-03 21:50:22 | INFO | hw5.seq2seq | example hypothesis: 他們甚至不知道為什麼他們會這麼做 。\n",
            "2022-04-03 21:50:22 | INFO | hw5.seq2seq | example reference: 他們甚至不見得知道為什麼要這麼做\n",
            "2022-04-03 21:50:22 | INFO | hw5.seq2seq | validation loss:\t3.2407\n",
            "2022-04-03 21:50:22 | INFO | hw5.seq2seq | BLEU = 22.30 53.9/29.5/17.3/10.8 (BP = 0.955 ratio = 0.956 hyp_len = 106922 ref_len = 111811)\n",
            "2022-04-03 21:50:23 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/haoyu/Desktop/2022ML/hw5/checkpoints/transformer-big/checkpoint6.pt\n",
            "2022-04-03 21:50:23 | INFO | hw5.seq2seq | end of epoch 6\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "612fc067b68241c3b349aad4234c9045",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": "train epoch 7:   0%|          | 0/3266 [00:00<?, ?it/s]"
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-03 22:15:58 | INFO | hw5.seq2seq | training loss: 3.8753\n",
            "2022-04-03 22:15:58 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "16f8c9dfc3fc433799463c42bb9ab612",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": "validation:   0%|          | 0/40 [00:00<?, ?it/s]"
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-03 22:16:29 | INFO | hw5.seq2seq | example source: how many children did you have ?\n",
            "2022-04-03 22:16:29 | INFO | hw5.seq2seq | example hypothesis: 你有多少個孩子 ?\n",
            "2022-04-03 22:16:29 | INFO | hw5.seq2seq | example reference: 你生了幾個小孩 ?\n",
            "2022-04-03 22:16:29 | INFO | hw5.seq2seq | validation loss:\t3.1949\n",
            "2022-04-03 22:16:29 | INFO | hw5.seq2seq | BLEU = 22.45 54.4/30.0/17.7/11.0 (BP = 0.946 ratio = 0.948 hyp_len = 105973 ref_len = 111811)\n",
            "2022-04-03 22:16:30 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/haoyu/Desktop/2022ML/hw5/checkpoints/transformer-big/checkpoint7.pt\n",
            "2022-04-03 22:16:30 | INFO | hw5.seq2seq | end of epoch 7\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "efb3dddaacde4c2ea4eca4af7133a94e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": "train epoch 8:   0%|          | 0/3266 [00:00<?, ?it/s]"
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-03 22:42:05 | INFO | hw5.seq2seq | training loss: 3.8306\n",
            "2022-04-03 22:42:05 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1f6d634bfcb94b49aee90ef42e188ab1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": "validation:   0%|          | 0/40 [00:00<?, ?it/s]"
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-03 22:42:36 | INFO | hw5.seq2seq | example source: i was sometimes left out of the questions asked of all other candidates and out of coverage about the elections .\n",
            "2022-04-03 22:42:36 | INFO | hw5.seq2seq | example hypothesis: 我有時會被問到其他候選人的問題 , 像是選舉的問題 。\n",
            "2022-04-03 22:42:36 | INFO | hw5.seq2seq | example reference: 在必答題的部分 , 有時候我會被遺漏 , 同樣的事情也發生在關於選舉的報導上 。\n",
            "2022-04-03 22:42:36 | INFO | hw5.seq2seq | validation loss:\t3.1645\n",
            "2022-04-03 22:42:36 | INFO | hw5.seq2seq | BLEU = 23.03 55.5/30.9/18.4/11.6 (BP = 0.937 ratio = 0.939 hyp_len = 104962 ref_len = 111811)\n",
            "2022-04-03 22:42:37 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/haoyu/Desktop/2022ML/hw5/checkpoints/transformer-big/checkpoint8.pt\n",
            "2022-04-03 22:42:37 | INFO | hw5.seq2seq | end of epoch 8\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cf0502d56f4549cb884eebd13838950a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": "train epoch 9:   0%|          | 0/3266 [00:00<?, ?it/s]"
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-03 23:08:16 | INFO | hw5.seq2seq | training loss: 3.7919\n",
            "2022-04-03 23:08:16 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cc2bccea0db4456ba3ad3e07228bb9fd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": "validation:   0%|          | 0/40 [00:00<?, ?it/s]"
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-03 23:08:48 | INFO | hw5.seq2seq | example source: now it is illegal for fathers to sell their daughters into servitude .\n",
            "2022-04-03 23:08:48 | INFO | hw5.seq2seq | example hypothesis: 現在是非法的 , 父親賣掉自己的女兒 , 賣掉女兒 。\n",
            "2022-04-03 23:08:48 | INFO | hw5.seq2seq | example reference: 現在當地的父親不能將自己的女兒賣去當奴隸 , 否則違法 。\n",
            "2022-04-03 23:08:48 | INFO | hw5.seq2seq | validation loss:\t3.1441\n",
            "2022-04-03 23:08:48 | INFO | hw5.seq2seq | BLEU = 23.32 52.9/29.4/17.4/10.9 (BP = 1.000 ratio = 1.014 hyp_len = 113414 ref_len = 111811)\n",
            "2022-04-03 23:08:49 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/haoyu/Desktop/2022ML/hw5/checkpoints/transformer-big/checkpoint9.pt\n",
            "2022-04-03 23:08:49 | INFO | hw5.seq2seq | end of epoch 9\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fa7642377cf342ad8ccaba51204b5251",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": "train epoch 10:   0%|          | 0/3266 [00:00<?, ?it/s]"
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-03 23:34:28 | INFO | hw5.seq2seq | training loss: 3.7589\n",
            "2022-04-03 23:34:28 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "99c7bb02a8a6492b97689eb59eee6b9b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": "validation:   0%|          | 0/40 [00:00<?, ?it/s]"
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-03 23:34:59 | INFO | hw5.seq2seq | example source: i got infected when i was 17 , when , as a student of the design college , i encountered adults who actually believed in my ideas , challenged me and had lots of cups of chai with me .\n",
            "2022-04-03 23:34:59 | INFO | hw5.seq2seq | example hypothesis: 我17歲時被感染 , 身為設計學院的學生 , 我遇到那些相信我的想法、挑戰我和很多人聊天的大人 。\n",
            "2022-04-03 23:34:59 | INFO | hw5.seq2seq | example reference: 我是17歲受到 「 感染 」 的當時我主修設計有些大人很認同我的設計理念還會提出許多疑問、和我邊喝茶邊討論\n",
            "2022-04-03 23:34:59 | INFO | hw5.seq2seq | validation loss:\t3.1204\n",
            "2022-04-03 23:34:59 | INFO | hw5.seq2seq | BLEU = 23.75 55.6/31.2/18.7/11.8 (BP = 0.956 ratio = 0.957 hyp_len = 106998 ref_len = 111811)\n",
            "2022-04-03 23:35:00 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/haoyu/Desktop/2022ML/hw5/checkpoints/transformer-big/checkpoint10.pt\n",
            "2022-04-03 23:35:00 | INFO | hw5.seq2seq | end of epoch 10\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cd35621c80224bd4a6c0543949cdf28e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": "train epoch 11:   0%|          | 0/3266 [00:00<?, ?it/s]"
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-04 00:00:39 | INFO | hw5.seq2seq | training loss: 3.7327\n",
            "2022-04-04 00:00:39 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d2f6e53223dd409e83670bcfee490bb5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": "validation:   0%|          | 0/40 [00:00<?, ?it/s]"
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-04 00:01:10 | INFO | hw5.seq2seq | example source: a tiny subset of them choose a profoundly nihilistic route , which is they die in the act of killing as many people as possible .\n",
            "2022-04-04 00:01:10 | INFO | hw5.seq2seq | example hypothesis: 其中一小部分是他們選擇了一條深深的nih路線 , 也就是他們盡可能地殺盡盡盡可能多的人 。\n",
            "2022-04-04 00:01:10 | INFO | hw5.seq2seq | example reference: 當中一小群人採取一種極端虛無主義的路線就是他們死的時候也要盡可能殺死愈多人愈好\n",
            "2022-04-04 00:01:10 | INFO | hw5.seq2seq | validation loss:\t3.1048\n",
            "2022-04-04 00:01:10 | INFO | hw5.seq2seq | BLEU = 23.92 55.3/31.0/18.5/11.7 (BP = 0.970 ratio = 0.970 hyp_len = 108507 ref_len = 111811)\n",
            "2022-04-04 00:01:11 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/haoyu/Desktop/2022ML/hw5/checkpoints/transformer-big/checkpoint11.pt\n",
            "2022-04-04 00:01:11 | INFO | hw5.seq2seq | end of epoch 11\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ffa9becc2fc044dd882c0212e8359835",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": "train epoch 12:   0%|          | 0/3266 [00:00<?, ?it/s]"
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-04 00:26:50 | INFO | hw5.seq2seq | training loss: 3.7072\n",
            "2022-04-04 00:26:50 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6511771033314aa4b4aaedbf2135f6c4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": "validation:   0%|          | 0/40 [00:00<?, ?it/s]"
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-04 00:27:21 | INFO | hw5.seq2seq | example source: those who were intended to receive the data , 74 percent of them received it .\n",
            "2022-04-04 00:27:21 | INFO | hw5.seq2seq | example hypothesis: 那些想要取得資料的人 , 74%得到了資料 。\n",
            "2022-04-04 00:27:21 | INFO | hw5.seq2seq | example reference: 那些希望收到資訊的人其中有百分之七十四確實收到\n",
            "2022-04-04 00:27:21 | INFO | hw5.seq2seq | validation loss:\t3.0952\n",
            "2022-04-04 00:27:21 | INFO | hw5.seq2seq | BLEU = 24.31 56.0/31.5/19.0/12.1 (BP = 0.962 ratio = 0.963 hyp_len = 107663 ref_len = 111811)\n",
            "2022-04-04 00:27:22 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/haoyu/Desktop/2022ML/hw5/checkpoints/transformer-big/checkpoint12.pt\n",
            "2022-04-04 00:27:22 | INFO | hw5.seq2seq | end of epoch 12\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "94d3c4578b944e21b2f53312dd4a0497",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": "train epoch 13:   0%|          | 0/3266 [00:00<?, ?it/s]"
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-04 00:52:58 | INFO | hw5.seq2seq | training loss: 3.6852\n",
            "2022-04-04 00:52:58 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d83c435fde604b5da3cfb9331db957c4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": "validation:   0%|          | 0/40 [00:00<?, ?it/s]"
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-04 00:53:29 | INFO | hw5.seq2seq | example source: this kind of effervescent fully charged sense of soup that delivers ideas .\n",
            "2022-04-04 00:53:29 | INFO | hw5.seq2seq | example hypothesis: 這種香氣的味道充斥著味道 , 會傳遞想法 。\n",
            "2022-04-04 00:53:29 | INFO | hw5.seq2seq | example reference: 這種能傳遞構想的滾燙 , 有味的湯汁 。\n",
            "2022-04-04 00:53:29 | INFO | hw5.seq2seq | validation loss:\t3.0768\n",
            "2022-04-04 00:53:29 | INFO | hw5.seq2seq | BLEU = 24.02 55.5/31.1/18.7/11.9 (BP = 0.964 ratio = 0.965 hyp_len = 107896 ref_len = 111811)\n",
            "2022-04-04 00:53:29 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/haoyu/Desktop/2022ML/hw5/checkpoints/transformer-big/checkpoint13.pt\n",
            "2022-04-04 00:53:29 | INFO | hw5.seq2seq | end of epoch 13\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cbc6f920e3604e659db8b275dfc61145",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": "train epoch 14:   0%|          | 0/3266 [00:00<?, ?it/s]"
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-04 01:19:09 | INFO | hw5.seq2seq | training loss: 3.6666\n",
            "2022-04-04 01:19:09 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8f549da195604e748f2026ecc83a2ff3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": "validation:   0%|          | 0/40 [00:00<?, ?it/s]"
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-04 01:19:40 | INFO | hw5.seq2seq | example source: until we live in a society where every human is assured dignity in their labor so that they can work to live well , not only work to survive , there will always be an element of those who seek the open road as a means of escape , of liberation and , of course , of rebellion .\n",
            "2022-04-04 01:19:40 | INFO | hw5.seq2seq | example hypothesis: 直到我們居住的這個社會 , 每個人的努力都被尊嚴保障 , 這樣他們才能好好活著 , 不僅僅是為了生存 , 還是為了那些尋求開放的道路作為逃離自由的途徑 , 當然 , 還有反抗的 。\n",
            "2022-04-04 01:19:40 | INFO | hw5.seq2seq | example reference: 直到我們居住在一種人人的勞動都保有尊嚴的社會 , 他們能靠工作讓生活過得更好 , 不只是為了存活而工作 , 屆時 , 永遠都會有一群人將追尋開闊的道路做為一種逃離、解放 , 當然 , 還有反抗的方式 。\n",
            "2022-04-04 01:19:40 | INFO | hw5.seq2seq | validation loss:\t3.0640\n",
            "2022-04-04 01:19:40 | INFO | hw5.seq2seq | BLEU = 24.55 54.8/30.8/18.6/11.8 (BP = 0.995 ratio = 0.995 hyp_len = 111223 ref_len = 111811)\n",
            "2022-04-04 01:19:41 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/haoyu/Desktop/2022ML/hw5/checkpoints/transformer-big/checkpoint14.pt\n",
            "2022-04-04 01:19:41 | INFO | hw5.seq2seq | end of epoch 14\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5a925813ea324e77bc94f5c5dc8c6c5c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": "train epoch 15:   0%|          | 0/3266 [00:00<?, ?it/s]"
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-04 01:45:20 | INFO | hw5.seq2seq | training loss: 3.6491\n",
            "2022-04-04 01:45:20 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c83e9596a81146e58b70d7ce24a453fe",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": "validation:   0%|          | 0/40 [00:00<?, ?it/s]"
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-04 01:45:51 | INFO | hw5.seq2seq | example source: i mean , can you imagine ? e . t . 's phoning home , and i'm not , like , there ? you know , horrible !\n",
            "2022-04-04 01:45:51 | INFO | hw5.seq2seq | example hypothesis: 我的意思是 , 你能想像嗎 ? t.t.在回家的路上 , 我不是 , 真的很糟糕 !\n",
            "2022-04-04 01:45:51 | INFO | hw5.seq2seq | example reference: 我的意思是 , 你能想像嗎 ? et打電話到我家 , 而我竟然不在 ? 知道了吧 , 太糟糕了 !\n",
            "2022-04-04 01:45:51 | INFO | hw5.seq2seq | validation loss:\t3.0617\n",
            "2022-04-04 01:45:51 | INFO | hw5.seq2seq | BLEU = 24.69 55.9/31.5/19.1/12.2 (BP = 0.977 ratio = 0.977 hyp_len = 109218 ref_len = 111811)\n",
            "2022-04-04 01:45:51 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/haoyu/Desktop/2022ML/hw5/checkpoints/transformer-big/checkpoint15.pt\n",
            "2022-04-04 01:45:51 | INFO | hw5.seq2seq | end of epoch 15\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "12791fd233c44cc7a099e11a637a6319",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": "train epoch 16:   0%|          | 0/3266 [00:00<?, ?it/s]"
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-04 02:11:31 | INFO | hw5.seq2seq | training loss: 3.6324\n",
            "2022-04-04 02:11:31 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fd983018dd5f49f797ef009021b917ef",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": "validation:   0%|          | 0/40 [00:00<?, ?it/s]"
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-04 02:12:02 | INFO | hw5.seq2seq | example source: it's the idea that some people don't believe in an afterlife .\n",
            "2022-04-04 02:12:02 | INFO | hw5.seq2seq | example hypothesis: 有些人不相信安息日 。\n",
            "2022-04-04 02:12:02 | INFO | hw5.seq2seq | example reference: 主要構想是有些人不信有來世 。\n",
            "2022-04-04 02:12:02 | INFO | hw5.seq2seq | validation loss:\t3.0494\n",
            "2022-04-04 02:12:02 | INFO | hw5.seq2seq | BLEU = 24.49 56.0/31.6/19.1/12.2 (BP = 0.965 ratio = 0.966 hyp_len = 107967 ref_len = 111811)\n",
            "2022-04-04 02:12:02 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/haoyu/Desktop/2022ML/hw5/checkpoints/transformer-big/checkpoint16.pt\n",
            "2022-04-04 02:12:02 | INFO | hw5.seq2seq | end of epoch 16\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6080bf19e69f4268aaaed8a3914682a8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": "train epoch 17:   0%|          | 0/3266 [00:00<?, ?it/s]"
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-04 02:37:41 | INFO | hw5.seq2seq | training loss: 3.6150\n",
            "2022-04-04 02:37:41 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4bc71f1102a84202888df67264a550aa",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": "validation:   0%|          | 0/40 [00:00<?, ?it/s]"
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-04 02:38:11 | INFO | hw5.seq2seq | example source: stairs add enormous drama .\n",
            "2022-04-04 02:38:11 | INFO | hw5.seq2seq | example hypothesis: 樓梯有著巨大的戲劇效果 。\n",
            "2022-04-04 02:38:11 | INFO | hw5.seq2seq | example reference: 樓梯能帶來很大的戲劇效果 。\n",
            "2022-04-04 02:38:11 | INFO | hw5.seq2seq | validation loss:\t3.0446\n",
            "2022-04-04 02:38:11 | INFO | hw5.seq2seq | BLEU = 24.38 56.6/32.0/19.4/12.3 (BP = 0.951 ratio = 0.952 hyp_len = 106485 ref_len = 111811)\n",
            "2022-04-04 02:38:12 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/haoyu/Desktop/2022ML/hw5/checkpoints/transformer-big/checkpoint17.pt\n",
            "2022-04-04 02:38:12 | INFO | hw5.seq2seq | end of epoch 17\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cdba89666fbb48f8a850d43393c4e580",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": "train epoch 18:   0%|          | 0/3266 [00:00<?, ?it/s]"
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-04 03:03:47 | INFO | hw5.seq2seq | training loss: 3.6019\n",
            "2022-04-04 03:03:47 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ad17e9232af04264ad4599612644da7c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": "validation:   0%|          | 0/40 [00:00<?, ?it/s]"
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-04 03:04:18 | INFO | hw5.seq2seq | example source: this is a kind of sugar that is actually found at certain levels on all of the cells in your body .\n",
            "2022-04-04 03:04:18 | INFO | hw5.seq2seq | example hypothesis: 這是一種糖 , 可以在你體內的特定細胞上發現 。\n",
            "2022-04-04 03:04:18 | INFO | hw5.seq2seq | example reference: 實際上 , 在你體內的所有細胞上 , 都可找到一定量的這種糖 。\n",
            "2022-04-04 03:04:18 | INFO | hw5.seq2seq | validation loss:\t3.0360\n",
            "2022-04-04 03:04:18 | INFO | hw5.seq2seq | BLEU = 24.67 56.3/31.8/19.2/12.3 (BP = 0.968 ratio = 0.968 hyp_len = 108250 ref_len = 111811)\n",
            "2022-04-04 03:04:18 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/haoyu/Desktop/2022ML/hw5/checkpoints/transformer-big/checkpoint18.pt\n",
            "2022-04-04 03:04:18 | INFO | hw5.seq2seq | end of epoch 18\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7699dd0ea84548cbb86c2dcce6dc95a3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": "train epoch 19:   0%|          | 0/3266 [00:00<?, ?it/s]"
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-04 03:29:54 | INFO | hw5.seq2seq | training loss: 3.5888\n",
            "2022-04-04 03:29:54 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "677789450e0a4ac3af06489d4b879081",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": "validation:   0%|          | 0/40 [00:00<?, ?it/s]"
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-04 03:30:24 | INFO | hw5.seq2seq | example source: now , it's not a false color view .\n",
            "2022-04-04 03:30:24 | INFO | hw5.seq2seq | example hypothesis: 這不是虛假的顏色 。\n",
            "2022-04-04 03:30:24 | INFO | hw5.seq2seq | example reference: 畫面上的這些顏色並不是假的 。\n",
            "2022-04-04 03:30:24 | INFO | hw5.seq2seq | validation loss:\t3.0332\n",
            "2022-04-04 03:30:24 | INFO | hw5.seq2seq | BLEU = 24.60 56.4/32.0/19.4/12.4 (BP = 0.957 ratio = 0.958 hyp_len = 107078 ref_len = 111811)\n",
            "2022-04-04 03:30:25 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/haoyu/Desktop/2022ML/hw5/checkpoints/transformer-big/checkpoint19.pt\n",
            "2022-04-04 03:30:25 | INFO | hw5.seq2seq | end of epoch 19\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "13b5cd2cc1f9439696e5b81a33e8651d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": "train epoch 20:   0%|          | 0/3266 [00:00<?, ?it/s]"
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-04 03:56:03 | INFO | hw5.seq2seq | training loss: 3.5749\n",
            "2022-04-04 03:56:03 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9bbcc40d739c4b56ba984297b7f6b681",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": "validation:   0%|          | 0/40 [00:00<?, ?it/s]"
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-04 03:56:35 | INFO | hw5.seq2seq | example source: this everexpanding infrastructural matrix already consists of 64 million kilometers of roads , four million kilometers of railways , two million kilometers of pipelines and one million kilometers of internet cables .\n",
            "2022-04-04 03:56:35 | INFO | hw5.seq2seq | example hypothesis: 這個不斷擴大的基礎矩陣已經包含了六千四百萬公里的道路 , 四百萬公里的道路 , 四百萬公里的道路 , 兩百萬公里的道路 , 以及兩百萬公里的道路 , 以及一百萬公里的網路纜線 。\n",
            "2022-04-04 03:56:35 | INFO | hw5.seq2seq | example reference: 這個一直不斷擴張的基礎建設矩陣 , 已經建造了6400萬公里的道路、400萬公里的鐵路、200萬公里的管道線路以及100萬公里的網路纜線 。\n",
            "2022-04-04 03:56:35 | INFO | hw5.seq2seq | validation loss:\t3.0305\n",
            "2022-04-04 03:56:35 | INFO | hw5.seq2seq | BLEU = 24.78 54.7/30.9/18.8/12.1 (BP = 0.996 ratio = 0.996 hyp_len = 111346 ref_len = 111811)\n",
            "2022-04-04 03:56:36 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/haoyu/Desktop/2022ML/hw5/checkpoints/transformer-big/checkpoint20.pt\n",
            "2022-04-04 03:56:36 | INFO | hw5.seq2seq | end of epoch 20\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "af71d2d475b64001a45a9da0a167b0aa",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": "train epoch 21:   0%|          | 0/3266 [00:00<?, ?it/s]"
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-04 04:22:14 | INFO | hw5.seq2seq | training loss: 3.5636\n",
            "2022-04-04 04:22:14 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fb98d2e9258a4429b2ed8a6a56ab347f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": "validation:   0%|          | 0/40 [00:00<?, ?it/s]"
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-04 04:22:44 | INFO | hw5.seq2seq | example source: so , in the lab , i've been creating art as a way to help me design better experiences for bleedingedge technology .\n",
            "2022-04-04 04:22:44 | INFO | hw5.seq2seq | example hypothesis: 所以 , 在實驗室裡 , 我創造了藝術 , 來協助我設計出更好的體驗 , 來協助我設計出更好的體驗 。\n",
            "2022-04-04 04:22:44 | INFO | hw5.seq2seq | example reference: 所以 , 在實驗室中 , 我一直在創造藝術 , 藝術協助我為最先進的技術設計出更佳的體驗 。\n",
            "2022-04-04 04:22:44 | INFO | hw5.seq2seq | validation loss:\t3.0231\n",
            "2022-04-04 04:22:44 | INFO | hw5.seq2seq | BLEU = 24.85 56.7/32.2/19.7/12.7 (BP = 0.957 ratio = 0.958 hyp_len = 107096 ref_len = 111811)\n",
            "2022-04-04 04:22:45 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/haoyu/Desktop/2022ML/hw5/checkpoints/transformer-big/checkpoint21.pt\n",
            "2022-04-04 04:22:45 | INFO | hw5.seq2seq | end of epoch 21\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d945b80ae7a14011bdf951fd08caed02",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": "train epoch 22:   0%|          | 0/3266 [00:00<?, ?it/s]"
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-04 04:48:23 | INFO | hw5.seq2seq | training loss: 3.5542\n",
            "2022-04-04 04:48:23 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ea125422b92c40c39709b06f11ca9880",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": "validation:   0%|          | 0/40 [00:00<?, ?it/s]"
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-04 04:48:54 | INFO | hw5.seq2seq | example source: how did you make all these people pay for music ? \"\n",
            "2022-04-04 04:48:54 | INFO | hw5.seq2seq | example hypothesis: 你是如何讓這些人為音樂付錢的 ? 」\n",
            "2022-04-04 04:48:54 | INFO | hw5.seq2seq | example reference: 你是如何讓這麼多人為音樂付錢的 ?\n",
            "2022-04-04 04:48:54 | INFO | hw5.seq2seq | validation loss:\t3.0165\n",
            "2022-04-04 04:48:54 | INFO | hw5.seq2seq | BLEU = 25.18 55.5/31.4/19.2/12.4 (BP = 0.993 ratio = 0.993 hyp_len = 110979 ref_len = 111811)\n",
            "2022-04-04 04:48:55 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/haoyu/Desktop/2022ML/hw5/checkpoints/transformer-big/checkpoint22.pt\n",
            "2022-04-04 04:48:55 | INFO | hw5.seq2seq | end of epoch 22\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "84a5cff5206b41e5a3be595836718db2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": "train epoch 23:   0%|          | 0/3266 [00:00<?, ?it/s]"
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-04 05:14:34 | INFO | hw5.seq2seq | training loss: 3.5406\n",
            "2022-04-04 05:14:34 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8ab8aac6872c481ba4d63a73c9c821fa",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": "validation:   0%|          | 0/40 [00:00<?, ?it/s]"
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-04 05:15:05 | INFO | hw5.seq2seq | example source: well , this is where dorothy and the mango tree charity that supports her comes in .\n",
            "2022-04-04 05:15:05 | INFO | hw5.seq2seq | example hypothesis: 嗯 , 這就是多蘿西和曼徹斯特慈善機構支持她來到這裡的 。\n",
            "2022-04-04 05:15:05 | INFO | hw5.seq2seq | example reference: 很好 , 這就是桃樂思跟支持她生計的芒果樹基金會來到我談話重點的時刻 。\n",
            "2022-04-04 05:15:05 | INFO | hw5.seq2seq | validation loss:\t3.0214\n",
            "2022-04-04 05:15:05 | INFO | hw5.seq2seq | BLEU = 25.14 56.9/32.5/19.9/12.9 (BP = 0.959 ratio = 0.960 hyp_len = 107287 ref_len = 111811)\n",
            "2022-04-04 05:15:05 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/haoyu/Desktop/2022ML/hw5/checkpoints/transformer-big/checkpoint23.pt\n",
            "2022-04-04 05:15:05 | INFO | hw5.seq2seq | end of epoch 23\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "feabdb27b4214179bdddb5599b3c5e5e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": "train epoch 24:   0%|          | 0/3266 [00:00<?, ?it/s]"
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-04 05:40:44 | INFO | hw5.seq2seq | training loss: 3.5343\n",
            "2022-04-04 05:40:44 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1ad713328540417987c04758ab43d783",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": "validation:   0%|          | 0/40 [00:00<?, ?it/s]"
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-04 05:41:15 | INFO | hw5.seq2seq | example source: these are archival tags .\n",
            "2022-04-04 05:41:15 | INFO | hw5.seq2seq | example hypothesis: 這些是阿基米德的標籤 。\n",
            "2022-04-04 05:41:15 | INFO | hw5.seq2seq | example reference: 這些是衛星檔案標幟\n",
            "2022-04-04 05:41:15 | INFO | hw5.seq2seq | validation loss:\t3.0100\n",
            "2022-04-04 05:41:15 | INFO | hw5.seq2seq | BLEU = 24.78 56.5/32.1/19.5/12.6 (BP = 0.959 ratio = 0.960 hyp_len = 107362 ref_len = 111811)\n",
            "2022-04-04 05:41:16 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/haoyu/Desktop/2022ML/hw5/checkpoints/transformer-big/checkpoint24.pt\n",
            "2022-04-04 05:41:16 | INFO | hw5.seq2seq | end of epoch 24\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c63085600ea247928a95d38e40bc876d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": "train epoch 25:   0%|          | 0/3266 [00:00<?, ?it/s]"
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-04 06:06:51 | INFO | hw5.seq2seq | training loss: 3.5250\n",
            "2022-04-04 06:06:51 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a5f9a3d235de4ab88db12a00b9f916fc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": "validation:   0%|          | 0/40 [00:00<?, ?it/s]"
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-04 06:07:21 | INFO | hw5.seq2seq | example source: i however think that we could do a better job , and we could do a better job by embracing isirika .\n",
            "2022-04-04 06:07:21 | INFO | hw5.seq2seq | example hypothesis: 無論如何我們都能做得更好 , 我們都能透過擁抱 「 依西里卡 」 來做更好的工作 。\n",
            "2022-04-04 06:07:21 | INFO | hw5.seq2seq | example reference: 然而我認為我們可以做得更好 , 透過擁抱 「 依西里卡 」 做得更好 。\n",
            "2022-04-04 06:07:21 | INFO | hw5.seq2seq | validation loss:\t3.0153\n",
            "2022-04-04 06:07:21 | INFO | hw5.seq2seq | BLEU = 24.73 57.2/32.5/19.7/12.7 (BP = 0.946 ratio = 0.948 hyp_len = 105962 ref_len = 111811)\n",
            "2022-04-04 06:07:22 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/haoyu/Desktop/2022ML/hw5/checkpoints/transformer-big/checkpoint25.pt\n",
            "2022-04-04 06:07:22 | INFO | hw5.seq2seq | end of epoch 25\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e2369ec3ba434ec488fc32dd3e9a36d3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": "train epoch 26:   0%|          | 0/3266 [00:00<?, ?it/s]"
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-04 06:32:59 | INFO | hw5.seq2seq | training loss: 3.5157\n",
            "2022-04-04 06:32:59 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "41909404fd4947c6978b599732eacc40",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": "validation:   0%|          | 0/40 [00:00<?, ?it/s]"
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-04 06:33:29 | INFO | hw5.seq2seq | example source: there are only two base words in all of english that have the letters \" doub \" : one is doubt , and the other is double .\n",
            "2022-04-04 06:33:29 | INFO | hw5.seq2seq | example hypothesis: 只有兩個英文字母的字母有 「 懷疑 」 字母:一個是懷疑 , 另一個是雙關語 。\n",
            "2022-04-04 06:33:29 | INFO | hw5.seq2seq | example reference: 在所有英語中 , 只有兩個基本詞有 「 d-o-u-b 」 的拼寫:其中一個是doubt、而另一個是double 。\n",
            "2022-04-04 06:33:29 | INFO | hw5.seq2seq | validation loss:\t3.0087\n",
            "2022-04-04 06:33:29 | INFO | hw5.seq2seq | BLEU = 25.00 57.1/32.6/19.9/12.9 (BP = 0.951 ratio = 0.952 hyp_len = 106457 ref_len = 111811)\n",
            "2022-04-04 06:33:30 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/haoyu/Desktop/2022ML/hw5/checkpoints/transformer-big/checkpoint26.pt\n",
            "2022-04-04 06:33:30 | INFO | hw5.seq2seq | end of epoch 26\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4da2946a2ed740a0af785363a5704ff2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": "train epoch 27:   0%|          | 0/3266 [00:00<?, ?it/s]"
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-04 06:59:08 | INFO | hw5.seq2seq | training loss: 3.5057\n",
            "2022-04-04 06:59:08 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2adecd547b9a48eda75310b2e4f1054e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": "validation:   0%|          | 0/40 [00:00<?, ?it/s]"
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-04 06:59:39 | INFO | hw5.seq2seq | example source: and that takes me to my final part , part three .\n",
            "2022-04-04 06:59:39 | INFO | hw5.seq2seq | example hypothesis: 這就帶到了我最後的部分 , 第三部分 。\n",
            "2022-04-04 06:59:39 | INFO | hw5.seq2seq | example reference: 而這帶我們來到了第三也是最後一部份 。\n",
            "2022-04-04 06:59:39 | INFO | hw5.seq2seq | validation loss:\t3.0046\n",
            "2022-04-04 06:59:39 | INFO | hw5.seq2seq | BLEU = 25.12 55.9/31.9/19.5/12.7 (BP = 0.976 ratio = 0.976 hyp_len = 109123 ref_len = 111811)\n",
            "2022-04-04 06:59:39 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/haoyu/Desktop/2022ML/hw5/checkpoints/transformer-big/checkpoint27.pt\n",
            "2022-04-04 06:59:39 | INFO | hw5.seq2seq | end of epoch 27\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "05b31635b6104d378e3b5010be5d4d86",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": "train epoch 28:   0%|          | 0/3266 [00:00<?, ?it/s]"
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-04 07:25:15 | INFO | hw5.seq2seq | training loss: 3.4986\n",
            "2022-04-04 07:25:15 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "edfa797401f04f27963ae56902b97783",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": "validation:   0%|          | 0/40 [00:00<?, ?it/s]"
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-04 07:25:46 | INFO | hw5.seq2seq | example source: now , for as interesting and remarkable a part of the plant as cbd is , it actually makes up a really tiny portion of the commercial market .\n",
            "2022-04-04 07:25:46 | INFO | hw5.seq2seq | example hypothesis: 如同其名 , 令人訝異的部分是 , 它其實是商業市場的一部分 。\n",
            "2022-04-04 07:25:46 | INFO | hw5.seq2seq | example reference: 雖然大麻二酚是這種植物中很有趣也很了不起的一部分 , 但它在商業市場中所佔的比例非常低 。\n",
            "2022-04-04 07:25:46 | INFO | hw5.seq2seq | validation loss:\t3.0082\n",
            "2022-04-04 07:25:46 | INFO | hw5.seq2seq | BLEU = 25.32 57.0/32.6/20.0/13.0 (BP = 0.962 ratio = 0.963 hyp_len = 107636 ref_len = 111811)\n",
            "2022-04-04 07:25:46 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/haoyu/Desktop/2022ML/hw5/checkpoints/transformer-big/checkpoint28.pt\n",
            "2022-04-04 07:25:46 | INFO | hw5.seq2seq | end of epoch 28\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4ee6184b19ce480b8b792ff1f616b389",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": "train epoch 29:   0%|          | 0/3266 [00:00<?, ?it/s]"
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-04 07:51:26 | INFO | hw5.seq2seq | training loss: 3.4939\n",
            "2022-04-04 07:51:26 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e6ca0ec3c6b44bd492f1f949d68db008",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": "validation:   0%|          | 0/40 [00:00<?, ?it/s]"
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-04 07:51:57 | INFO | hw5.seq2seq | example source: who would ever have thought you could have your own eeg at your home , tagged to a very nice alarm clock , by the way ?\n",
            "2022-04-04 07:51:57 | INFO | hw5.seq2seq | example hypothesis: 誰會想在家裡安裝你的腦電波設備 ? 順便提一下 , 沒錯 , 沒錯 。\n",
            "2022-04-04 07:51:57 | INFO | hw5.seq2seq | example reference: 誰想到過在自己家裡 , 可以有個附腦波監測器的鬧鐘 。\n",
            "2022-04-04 07:51:57 | INFO | hw5.seq2seq | validation loss:\t2.9972\n",
            "2022-04-04 07:51:57 | INFO | hw5.seq2seq | BLEU = 25.40 55.1/31.4/19.3/12.5 (BP = 0.998 ratio = 0.998 hyp_len = 111624 ref_len = 111811)\n",
            "2022-04-04 07:51:58 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/haoyu/Desktop/2022ML/hw5/checkpoints/transformer-big/checkpoint29.pt\n",
            "2022-04-04 07:51:58 | INFO | hw5.seq2seq | end of epoch 29\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a266ede97a464d7987909309bea76fa3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": "train epoch 30:   0%|          | 0/3266 [00:00<?, ?it/s]"
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-04 08:17:36 | INFO | hw5.seq2seq | training loss: 3.4845\n",
            "2022-04-04 08:17:36 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "104cec5770a04c85823d070ae3b3504b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": "validation:   0%|          | 0/40 [00:00<?, ?it/s]"
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-04 08:18:06 | INFO | hw5.seq2seq | example source: india is food selfsufficient and in the past decade has actually been exporting calories .\n",
            "2022-04-04 08:18:06 | INFO | hw5.seq2seq | example hypothesis: 印度是自給自足的 , 在過去十年裡 , 自給自足的事實正是出口卡路里 。\n",
            "2022-04-04 08:18:06 | INFO | hw5.seq2seq | example reference: 印度的食物能自給自足 , 且過去十年間都在對外出口卡路里 。\n",
            "2022-04-04 08:18:06 | INFO | hw5.seq2seq | validation loss:\t2.9982\n",
            "2022-04-04 08:18:06 | INFO | hw5.seq2seq | BLEU = 25.13 56.8/32.5/19.9/12.9 (BP = 0.957 ratio = 0.958 hyp_len = 107088 ref_len = 111811)\n",
            "2022-04-04 08:18:07 | INFO | hw5.seq2seq | saved epoch checkpoint: /home/haoyu/Desktop/2022ML/hw5/checkpoints/transformer-big/checkpoint30.pt\n",
            "2022-04-04 08:18:07 | INFO | hw5.seq2seq | end of epoch 30\n"
          ]
        }
      ],
      "source": [
        "epoch_itr = load_data_iterator(task, \"train\", config.start_epoch, config.max_tokens, config.num_workers)\n",
        "try_load_checkpoint(model, optimizer, name=config.resume)\n",
        "while epoch_itr.next_epoch_idx <= config.max_epoch:\n",
        "    # train for one epoch\n",
        "    train_one_epoch(epoch_itr, model, task, criterion, optimizer, config.accum_steps)\n",
        "    stats = validate_and_save(model, task, criterion, optimizer, epoch=epoch_itr.epoch)\n",
        "    logger.info(\"end of epoch {}\".format(epoch_itr.epoch))    \n",
        "    epoch_itr = load_data_iterator(task, \"train\", epoch_itr.next_epoch_idx, config.max_tokens, config.num_workers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Namespace(checkpoint_upper_bound=None, inputs=['./checkpoints/transformer-big'], num_epoch_checkpoints=5, num_update_checkpoints=None, output='./checkpoints/transformer-big/avg_last_5_checkpoint.pt')\n",
            "averaging checkpoints:  ['./checkpoints/transformer-big/checkpoint30.pt', './checkpoints/transformer-big/checkpoint29.pt', './checkpoints/transformer-big/checkpoint28.pt', './checkpoints/transformer-big/checkpoint27.pt', './checkpoints/transformer-big/checkpoint26.pt']\n",
            "Finished writing averaged checkpoint to ./checkpoints/transformer-big/avg_last_5_checkpoint.pt\n"
          ]
        }
      ],
      "source": [
        "# averaging a few checkpoints can have a similar effect to ensemble\n",
        "checkdir=config.savedir\n",
        "!python3 ./fairseq/scripts/average_checkpoints.py \\\n",
        "--inputs {checkdir} \\\n",
        "--num-epoch-checkpoints 5 \\\n",
        "--output {checkdir}/avg_last_5_checkpoint.pt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-04 08:18:10 | INFO | hw5.seq2seq | loaded checkpoint checkpoints/transformer-big/avg_last_5_checkpoint.pt: step=unknown loss=2.9982056617736816 bleu=25.128381350100387\n",
            "2022-04-04 08:18:10 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e968c188ae3a44e2a544fd37a11aaab7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": "validation:   0%|          | 0/40 [00:00<?, ?it/s]"
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-04 08:18:41 | INFO | hw5.seq2seq | example source: and i had an armored organization at that point .\n",
            "2022-04-04 08:18:41 | INFO | hw5.seq2seq | example hypothesis: 當時我有一個盔甲組織 。\n",
            "2022-04-04 08:18:41 | INFO | hw5.seq2seq | example reference: 當時我的團隊都是全副武裝 。\n",
            "2022-04-04 08:18:41 | INFO | hw5.seq2seq | validation loss:\t2.9735\n",
            "2022-04-04 08:18:41 | INFO | hw5.seq2seq | BLEU = 25.73 57.0/32.7/20.2/13.2 (BP = 0.969 ratio = 0.969 hyp_len = 108355 ref_len = 111811)\n"
          ]
        }
      ],
      "source": [
        "# checkpoint_last.pt : latest epoch\n",
        "# checkpoint_best.pt : highest validation bleu\n",
        "# avg_last_5_checkpoint.pt:　the average of last 5 epochs\n",
        "try_load_checkpoint(model, name=\"avg_last_5_checkpoint.pt\")\n",
        "validate(model, task, criterion, log_to_wandb=False)\n",
        "None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-04 10:55:12 | INFO | fairseq.data.data_utils | loaded 4,000 examples from: ./DATA/data-bin/ted2020_with_mono/test.en-zh.en\n",
            "2022-04-04 10:55:12 | INFO | fairseq.data.data_utils | loaded 4,000 examples from: ./DATA/data-bin/ted2020_with_mono/test.en-zh.zh\n",
            "2022-04-04 10:55:12 | INFO | fairseq.tasks.translation | ./DATA/data-bin/ted2020_with_mono test en-zh 4000 examples\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d3c2e47cc9214fbdb17c3fc4bf678625",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": "prediction:   0%|          | 0/32 [00:00<?, ?it/s]"
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "generate_prediction(model, task, outfile=\"./prediction_back.txt\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.8.10 64-bit",
      "name": "python3810jvsc74a57bd031f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "metadata": {
      "interpreter": {
        "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
      }
    },
    "orig_nbformat": 3
  },
  "nbformat": 4,
  "nbformat_minor": 0
}